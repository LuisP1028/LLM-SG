<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Part III: RAG Architectures // Study Guide</title>
<style>
    :root {
        --bg-color: #000000;
        --text-color: #00ff41;
        --accent-color: #00ff41;
        --dim-color: #003b00;
        --border-color: #00ff41;
        --font-main: 'Courier New', Courier, monospace;
        --font-header: 'Arial Black', Impact, sans-serif;
    }

    * { box-sizing: border-box; }

    body {
        margin: 0;
        padding: 0;
        background-color: var(--bg-color);
        color: var(--text-color);
        font-family: var(--font-main);
        line-height: 1.5;
        overflow-x: hidden;
    }

    /* --- VISUALS --- */
    .dither-layer {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        z-index: -1;
        background-image: radial-gradient(circle, #003b00 1px, transparent 1px);
        background-size: 4px 4px;
        opacity: 0.4;
    }

    .scanlines {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        background: linear-gradient(to bottom, rgba(0, 255, 65, 0), rgba(0, 255, 65, 0) 50%, rgba(0, 20, 0, 0.2) 50%, rgba(0, 20, 0, 0.2));
        background-size: 100% 4px;
        pointer-events: none;
        z-index: 9999;
    }

    .container {
        max-width: 900px;
        width: 100%;
        margin: 0 auto;
        padding: 40px 20px;
        border-left: 2px dashed var(--dim-color);
        border-right: 2px dashed var(--dim-color);
        background-color: rgba(0, 10, 0, 0.9);
        min-height: 100vh;
    }

    /* --- TYPOGRAPHY --- */
    h1 {
        font-family: var(--font-header);
        text-transform: uppercase;
        font-size: 2.5rem;
        border-bottom: 5px solid var(--accent-color);
        margin-bottom: 40px;
        color: var(--accent-color);
        text-align: center;
        text-shadow: 0px 0px 8px var(--accent-color);
        word-wrap: break-word;
    }

    h3 { margin-top: 0; color: var(--accent-color); text-transform: uppercase; font-size: 1.1rem; border-bottom: 1px dashed var(--dim-color); padding-bottom: 5px; }
    strong { color: var(--accent-color); text-decoration: none; font-weight: bold; }
    em { font-style: normal; color: #50c878; border-bottom: 1px dotted var(--dim-color); }

    /* --- ACCORDION STYLES --- */
    details.part {
        margin-bottom: 30px;
        border: 2px solid var(--border-color);
        background: #000;
        box-shadow: 6px 6px 0px var(--dim-color);
        transition: transform 0.1s;
    }
    details.part[open] { box-shadow: 4px 4px 0px var(--dim-color); transform: translate(2px, 2px); }
    details.part > summary {
        font-family: var(--font-header);
        font-size: 1.5rem;
        padding: 15px 20px;
        background-color: var(--accent-color);
        color: var(--bg-color);
        cursor: pointer;
        list-style: none;
        text-transform: uppercase;
        position: relative;
    }
    details.part > summary::-webkit-details-marker { display: none; }
    details.part > summary::after { content: '+'; position: absolute; right: 20px; font-weight: 900; }
    details.part[open] > summary::after { content: '-'; }
    .part-content { padding: 20px; border-top: 2px solid var(--border-color); }

    details.section {
        margin-bottom: 15px;
        border: 1px solid var(--dim-color);
        background: #050505;
    }
    details.section > summary {
        font-family: var(--font-main);
        font-weight: bold;
        padding: 12px;
        background: #0a0a0a;
        color: var(--text-color);
        cursor: pointer;
        list-style: none;
        border-bottom: 1px solid transparent;
        text-transform: uppercase;
        font-size: 1.1rem;
    }
    details.section > summary:hover { background: var(--dim-color); color: var(--accent-color); }
    details.section[open] > summary {
        border-bottom: 1px solid var(--dim-color);
        background: #0f0f0f;
        color: var(--accent-color);
        text-shadow: 0px 0px 5px var(--accent-color);
    }
    .section-content { padding: 20px; }

    .tree-root {
        border-left: 1px dashed var(--accent-color);
        margin-left: 1rem;
        padding-left: 1.5rem;
        position: relative;
    }

    details.nested-accordion {
        margin-bottom: 10px;
    }

    details.nested-accordion > summary {
        position: relative;
        list-style: none;
        cursor: pointer;
        padding: 0.5rem;
        transition: all 0.2s ease;
        border: 1px solid transparent;
        font-weight: bold;
        color: var(--text-color);
    }

    details.nested-accordion > summary::before {
        content: '';
        position: absolute;
        left: -1.5rem; 
        top: 50%;
        width: 1.5rem;
        border-bottom: 1px dashed var(--accent-color);
    }

    details.nested-accordion > summary:hover {
        background-color: var(--accent-color);
        color: black;
        border-color: var(--accent-color);
    }

    details.nested-accordion > summary::-webkit-details-marker { display: none; }
    details.nested-accordion > summary:focus { outline: none; }

    .nested-content {
        padding-left: 1rem;
        padding-top: 0.5rem;
        padding-bottom: 1rem;
        border-left: 1px dashed rgba(0, 255, 65, 0.3);
        margin-left: -0.8rem;
    }

    .tag {
        display: inline-block;
        border: 1px solid var(--accent-color);
        color: var(--bg-color);
        background-color: var(--accent-color);
        padding: 2px 8px;
        font-size: 0.75rem;
        font-weight: bold;
        margin-bottom: 15px;
        text-transform: uppercase;
    }

    .subsection {
        margin-bottom: 25px;
        border-left: 4px solid var(--dim-color);
        padding-left: 15px;
    }
    .subsection-title {
        background: var(--dim-color);
        color: var(--accent-color);
        padding: 2px 6px;
        font-weight: bold;
        text-transform: uppercase;
        display: inline-block;
        margin-bottom: 10px;
        font-size: 0.9rem;
    }

    p { margin-bottom: 12px; margin-top: 0; text-align: justify; }
    ul, ol { padding-left: 20px; margin-bottom: 15px; }
    li { margin-bottom: 5px; }

    .code-block {
        background: #020a02;
        border: 1px dashed var(--dim-color);
        padding: 10px;
        margin: 10px 0;
        font-family: 'Courier New', monospace;
        color: var(--accent-color);
        overflow-x: auto;
        white-space: pre-wrap;
    }

    code {
        background-color: var(--dim-color);
        color: #fff;
        padding: 2px 5px;
        font-family: 'Courier New', monospace;
        font-size: 0.9em;
    }

    .eye-btn {
        position: relative;
        width: 28px; height: 28px;
        background: #000;
        border: 1px solid var(--accent-color);
        cursor: pointer;
        padding: 4px;
        display: inline-flex;
        align-items: center; justify-content: center;
        margin-left: 8px;
        vertical-align: bottom;
        transition: transform 0.1s;
    }
    .eye-btn svg { width: 100%; height: 100%; fill: var(--accent-color); }
    .eye-btn:hover {
        background: var(--accent-color);
        transform: translate(-1px, -1px);
        box-shadow: 2px 2px 0px var(--dim-color);
    }
    .eye-btn:hover svg { fill: #000; }

    .retro-viewport {
        position: fixed;
        top: 50%; left: 50%;
        transform: translate(-50%, -50%);
        width: 80vw; height: 80vh;
        max-width: 900px; max-height: 700px;
        background-color: #000;
        border: 2px solid var(--accent-color);
        box-shadow: 0 0 50px rgba(0, 50, 0, 0.8);
        display: flex; flex-direction: column;
        z-index: 10000;
        visibility: hidden; opacity: 0;
        pointer-events: none;
        transition: opacity 0.2s;
        resize: both; overflow: hidden;
    }
    .retro-viewport.active { visibility: visible; opacity: 1; pointer-events: auto; }

    .vp-header {
        background: var(--accent-color);
        color: #000;
        padding: 5px 10px;
        font-weight: bold;
        font-family: var(--font-header);
        display: flex; justify-content: space-between;
        align-items: center;
        border-bottom: 2px solid #000;
        cursor: default;
    }
    .vp-close {
        background: #000; color: var(--accent-color);
        border: 1px solid #000; font-weight: 900; 
        cursor: pointer; font-family: var(--font-main);
    }
    .vp-close:hover { background: #fff; color: #000; }
    
    .vp-body { flex-grow: 1; position: relative; background: #000; }
    .vp-body iframe { width: 100%; height: 100%; border: none; }

    @media (max-width: 600px) {
        h1 { font-size: 1.8rem; border-bottom-width: 3px; }
        details.part > summary { font-size: 1.1rem; padding: 12px; }
        details.section > summary { font-size: 0.9rem; }
        .container { padding: 10px; border: none; }
        .part-content, .section-content { padding: 10px; }
        p { text-align: left; }
        .retro-viewport { width: 95vw; height: 60vh; }
    }
</style>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<div class="dither-layer"></div>
<div class="scanlines"></div>
<div class="container">
<h1>Part III: RAG Architectures</h1>

<div class="part-content">
        
        <details class="section">
            <summary>1. RAG Overview & Modules</summary>
            <div class="section-content">
                <p><strong>RAG:</strong> Pulling updated, tailored information from a vector store to concatenate as context to a user prompt, before feeding a foundational model.</p>
                <p><strong>A RAG system is composed of three main modules:</strong></p>
                <ul>
                    <li><strong>Ingestion pipeline:</strong> A batch or streaming pipeline used to populate the vector DB</li>
                    <li><strong>Retrieval pipeline:</strong> A module that queries the vector DB and retrieves relevant entries to the user’s input</li>
                    <li><strong>Generation pipeline:</strong> The layer that uses the retrieved data to augment the prompt and an LLM to generate answers</li>
                </ul>
                <div class="subsection">
                    <span class="subsection-title">HOW ARE MODULES CONNECTED OVERVIEW</span>
                    <ol>
                        <li>On the backend side, the ingestion pipeline runs either on a schedule or constantly to populate the vector DB with external data.</li>
                        <li>On the client side, the user asks a question.</li>
                        <li>The question is passed to the retrieval module, which preprocesses the user’s input and queries the vector DB.</li>
                        <li>The generation pipelines use a prompt template, user input, and retrieved context to create the prompt.</li>
                        <li>The prompt is passed to an LLM to generate the answer.</li>
                        <li>The answer is shown to the user.</li>
                    </ol>
                </div>
            </div>
        </details>

        <details class="section">
            <summary>2. RAG Ingestion: Walkthrough</summary>
            <div class="section-content">
                <ol>
                    <li><strong>Pull in the raw data (Data extraction/Ingestion):</strong> Collect documents from databases, PDFs, APIs, etc.</li>
                    <li><strong>Clean it up (Cleaning layer):</strong> Fix messy text: remove weird characters, strip URLs, delete headers/footers.</li>
                    <li><strong>Break into smaller pieces (Chunking/Splitting):</strong> Split large documents into 300-800 word chunks. Smart chunking keeps complete thoughts together.</li>
                    <li><strong>Turn into vectors (Embedding):</strong> Send chunks to an embedding model. Text becomes numerical vectors capturing meaning.</li>
                    <li><strong>Save in vector database (Loading):</strong> Store vectors + metadata (source URL, date, title) in a vector DB.</li>
                </ol>
                <p><strong>Result:</strong> When users ask questions, the system searches vectors for relevant chunks, adds them to the prompt, and the LLM generates accurate, grounded answers.</p>
                <p><strong>In short:</strong> Raw documents &rarr; cleaned &rarr; chunked &rarr; vectorized &rarr; stored &rarr; ready for AI retrieval.</p>
            </div>
        </details>

        <details class="section">
            <summary>3. RAG: Chunking</summary>
            <div class="section-content">
                <p>Chunking is the process of splitting large documents into smaller, discrete segments. In RAG systems, it is essential for:</p>
                <ul>
                    <li><strong>Context Limits:</strong> Ensures text fits within an LLM’s token constraints.</li>
                    <li><strong>Retrieval Precision:</strong> Improves semantic search by focusing on specific topics rather than broad documents.</li>
                    <li><strong>Relevancy:</strong> Helps the model generate accurate answers by providing only the most pertinent context.</li>
                </ul>
                <hr style="border: 0; border-top: 1px dashed var(--dim-color); margin: 20px 0;">
                
                <div class="tree-root">
                    <details class="nested-accordion">
                        <summary>Fixed-size Chunking</summary>
                        <div class="nested-content">
                            <button class="eye-btn" onclick="const v=document.querySelector('.retro-viewport'); v.querySelector('iframe').src='RAG/fixedsize_chunk'; v.classList.add('active');"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg></button>
                            <p>Slices text into hard segments based on a strict token or character limit (e.g., 512 tokens). While computationally efficient, it ignores semantic structure, often breaking sentences mid-thought. To mitigate context loss at the edges, a "sliding window" overlap (10-20%) is applied.</p>
                            <ul>
                                <li><em>Example:</em> Processing a raw server log file. You blindly cut the text every 500 characters, regardless of where the line ends.</li>
                                <li><em>When to use:</em> Ideal for uniform data streams or simple tasks where computational speed is the priority and semantic context is less critical (e.g., n-gram analysis).</li>
                            </ul>
                        </div>
                    </details>

                    <details class="nested-accordion">
                        <summary>Recursive Chunking</summary>
                        <div class="nested-content">
                            <button class="eye-btn" onclick="const v=document.querySelector('.retro-viewport'); v.querySelector('iframe').src='RAG/recursive_chunk.html'; v.classList.add('active');"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg></button>
                            <p>Iteratively breaks down text using a hierarchy of separators to fit specific size limits. It attempts to split by largest structural elements first (like paragraphs), then sentences, then words.</p>
                            <ul>
                                <li><em>Example:</em> An article is split first by double newlines (paragraphs). If a paragraph is still too big, it splits by periods (sentences).</li>
                                <li><em>When to use:</em> The "default" best choice for general prose, articles, and essays. It respects natural reading flow, keeping related ideas (paragraphs) intact.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="nested-accordion">
                        <summary>Syntactic Chunking</summary>
                        <div class="nested-content">
                            <button class="eye-btn" onclick="const v=document.querySelector('.retro-viewport'); v.querySelector('iframe').src='RAG/Syntactic_chunker.html'; v.classList.add('active');"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg></button>
                            <p>Leverages the distinct formatting logic of the source file to define boundaries. For code, it splits by functions or classes; for Markdown or HTML, it splits by headers or tags.</p>
                            <ul>
                                <li><em>Example:</em> Splitting a Python script by <code>def</code> blocks or an HTML file by <code>&lt;div&gt;</code> tags, ensuring no function is cut in half.</li>
                                <li><em>When to use:</em> Essential for structured documents like Code, Markdown, or JSON. It prevents breaking syntax, ensuring the retrieved chunk is valid and executable.</li>
                            </ul>
                        </div>
                    </details>

                    <details class="nested-accordion">
                        <summary>Semantic Chunking</summary>
                        <div class="nested-content">
                            <button class="eye-btn" onclick="const v=document.querySelector('.retro-viewport'); v.querySelector('iframe').src='RAG/semantic_chunker.html'; v.classList.add('active');"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg></button>
                            <p>Semantic chunking splits text into sentences and converts them to embeddings to assess similarity (usually via cosine). Chunks grow as long as sentences stay semantically close; once distance exceeds a threshold, a new chunk starts.</p>
                            <ul>
                                <li><em>Example:</em> A transcript moves from discussing "Weather" to "Stocks." The system detects the topic shift via embeddings and cuts the chunk exactly there.</li>
                                <li><em>When to use:</em> Best for RAG systems requiring high precision. It ensures a chunk contains only one distinct concept, reducing noise for the LLM.</li>
                            </ul>
                        </div>
                    </details>
                </div>
            </div>
        </details>

        <details class="section">
            <summary>4. RAG: Embedding</summary>
            <div class="section-content">
                <div class="tree-root">
                    <details class="nested-accordion">
                        <summary>Dense Embeddings (Bi-Encoders)</summary>
                        <div class="nested-content">
                            <p>Foundation for semantic search. Captures abstract meaning by collapsing an entire sequence [inputs, e.g document, sentence, paragraph] into a single "summary" vector.</p>
                            <p><strong>When to use:</strong> Use this when the intent matters more than specific words, though it may lose granular word-level nuance.</p>
                            
                            <div class="tree-root">
                                <details class="nested-accordion">
                                    <summary>How: Pooling Mechanisms</summary>
                                    <div class="nested-content">
                                        <div class="tree-root">
                                            <details class="nested-accordion">
                                                <summary>The Practical Workflow</summary>
                                                <div class="nested-content">
                                                    <p><strong>Contextual Encoding:</strong> Each token in your sequence (e.g., "The," "cat," "sat") starts as a raw vector. The Transformer uses Self-Attention to update these vectors based on their neighbors. By the final layer, the vector for "cat" isn't just "cat"—it contains numerical "traces" of "The" and "sat."</p>
                                                    <p><strong>The Accumulation (Pooling):</strong> Since you can’t compare a 5-word sentence to a 500-word document directly, you must reduce them to the same dimensions. This is done via:</p>
                                                    <ul>
                                                        <li><strong>CLS Pooling:</strong> Taking the vector of the special [CLS] (classification) token, which was designed to attend to every other token in the sequence.</li>
                                                        <li><strong>Mean/Max Pooling:</strong> Mathematically averaging all individual token vectors into one "centroid" vector.</li>
                                                    </ul>
                                                    <p><strong>The Resulting Vector:</strong> This single array of numbers (e.g., 768 dimensions) represents the "average" semantic direction of the sequence.</p>
                                                    <p><strong>Why Meaning is Lost:</strong> Because you are averaging (Mean Pooling) or selecting a representative (CLS), you lose positional granularity. You know the "flavor" of the sequence is about felines and posture, but the specific syntax—who did what to whom—gets blurred into a single mathematical coordinate.</p>
                                                </div>
                                            </details>

                                            <details class="nested-accordion">
                                                <summary>1. [CLS] Pooling
                                                    <button class="eye-btn" onclick="
                                                        const viewport = document.querySelector('.retro-viewport');
                                                        const iframe = viewport.querySelector('iframe');
                                                        iframe.src = 'RAG/cls.html';
                                                        viewport.classList.add('active');
                                                    ">
                                                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg>
                                                    </button>
                                                </summary>
                                                <div class="nested-content">
                                                    <p>A "dummy" token prepended to the sequence. Through self-attention, it acts as a designated note-taker, aggregating global context from every other token. By the final layer, this single vector serves as the complete embedding, forced by training to represent the entire sequence's semantics.</p>
                                                    <p>Choose this for classification if your model was specifically pre-trained to use it.</p>
                                                    
                                                </div>
                                            </details>

                                            <details class="nested-accordion">
                                                <summary>2. Mean Pooling
                                                    <button class="eye-btn" onclick="
                                                        const viewport = document.querySelector('.retro-viewport');
                                                        const iframe = viewport.querySelector('iframe');
                                                        iframe.src = 'RAG/mean_pooling.html';
                                                        viewport.classList.add('active');
                                                    ">
                                                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg>
                                                    </button>
                                                </summary>
                                                <div class="nested-content">
                                                    <p>The model calculates the element-wise [individual numerical values across the vector dimensions] average of all token vectors in the final layer. 
                                                    $$\vec{v}_{mean} = \frac{1}{n} \sum_{i=1}^{n} \vec{h}_i$$
                                                    This creates a "centroid" of the sequence. While excellent for capturing the overall topic, the averaging process can blur specific syntax and "who did what" details.</p>
                                                    <p>The Result: A single vector where each index is the arithmetic mean of that specific position across all tokens in the sequence.</p>
                                                    <p>Choose this for Semantic Search (RAG) or clustering; it’s more robust at capturing the overall "flavor" of the text.</p>
                                                </div>
                                            </details>

                                            <details class="nested-accordion">
                                                <summary>3. Max Pooling
                                                    <button class="eye-btn" onclick="
                                                        const viewport = document.querySelector('.retro-viewport');
                                                        const iframe = viewport.querySelector('iframe');
                                                        iframe.src = 'RAG/max_pool.html';
                                                        viewport.classList.add('active');
                                                    ">
                                                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg>
                                                    </button>
                                                </summary>
                                                <div class="nested-content">
                                                    <p>Condenses information into a fixed-size embedding, losing the individual identity of specific words to favor a global gist. The model selects the maximum value across each dimension from all token vectors. It is highly effective at identifying the presence of specific, high-impact keywords in a sequence.</p>
                                                    <p>Choose this for identifying specific, strong features or keywords.</p>
                                                </div>
                                            </details>
                                        </div>
                                    </div>
                                </details>
                            </div>
                        </div>
                    </details>

                    <details class="nested-accordion">
                        <summary>Sparse Embeddings</summary>
                        <div class="nested-content">
                            <p>Foundation for lexical search. High-dimensional vectors mapping to specific tokens. Essential for exact keyword matching.</p>
                            <p><strong>When to use:</strong> Use this for precision-heavy tasks where exact keyword matching is non-negotiable.</p>

                            <div class="tree-root">
                                <details class="nested-accordion">
                                    <summary>How: TF-IDF & Scoring</summary>
                                    <div class="nested-content">
                                        <div class="tree-root">
                                            <details class="nested-accordion">
                                                <summary>The Practical Workflow</summary>
                                                <div class="nested-content">
                                                    <p><strong>Vocab Mapping:</strong> The model has a fixed dictionary of, say, 30,000 words. Each word is assigned a specific "index" (a coordinate).</p>
                                                    <p><strong>Token Counting:</strong> It looks at your sequence and marks which words appear. If your sequence is "the cat sat," it flips the switch for the indices of "the," "cat," and "sat" from \(0\) to a positive number.</p>
                                                    <p><strong>Weighting (BM25/SPLADE):</strong> It doesn't just count; it weights.</p>
                                                    <ul>
                                                        <li><strong>Term Frequency:</strong> More mentions = higher weight.</li>
                                                        <li><strong>Inverted Document Frequency:</strong> Common words like "the" get penalized, while rare, "information-heavy" words like "photosynthesis" get boosted.</li>
                                                    </ul>
                                                    <p><strong>Result:</strong> You get a vector the size of the whole dictionary (30,000+). It is sparse because 29,997 of those numbers are zero; only the dimensions for your specific words hold values.</p>
                                                    <p><strong>Why it works:</strong> Instead of an "average meaning," this is a precise fingerprint. It excels at matching exact terms (like product SKUs or legal jargon) that dense embeddings might blur away.</p>
                                                </div>
                                            </details>

                                            <details class="nested-accordion">
                                                <summary>1. Term Frequency (TF)</summary>
                                                <div class="nested-content">
                                                    <p>TF measures how "loud" a word is within a specific sequence. If you are analyzing a 100-word paragraph about "Transformers," and the word "attention" appears 10 times, its TF is higher than a word that appears once.</p>
                                                    <p>In practice, we don't just use raw counts; we often use Log-normalization (\(\log(1 + \text{count})\)) to prevent a word that appears 100 times from being 100 times more "important" than a word appearing once. 
                                                    <p>It essentially answers: "How much is this document specifically about this term?"</p>
                                                </div>
                                            </details>

                                            <details class="nested-accordion">
                                                <summary>2. Inverted Document Frequency (IDF)</summary>
                                                <div class="nested-content">
                                                    <p>IDF measures how "rare" or "informative" a word is across your entire knowledge base. It is the "filter" that silences background noise.</p>
                                                    <ul>
                                                        <li><strong>Common Words:</strong> Words like "the," "is," or "of" appear in almost every document. Their IDF is near zero because they don't help distinguish one document from another.</li>
                                                        <li><strong>Rare Words:</strong> A technical term like "Backpropagation" might only appear in 5 out of 10,000 documents. Because it is rare, its IDF weight is very high.</li>
                                                    </ul>
                                                    <p><strong>The Math:</strong> It’s the log of (Total Documents / Documents containing the term). If a word is in every document, the fraction is 1, and \(\log(1) = 0\).</p>
                                                </div>
                                            </details>

                                            <details class="nested-accordion">
                                                <summary>3. The Combination (TF-IDF)</summary>
                                                <div class="nested-content">
                                                    <p>By multiplying \(TF \times IDF\), you get a score that is high only if a word is frequent in the current document but rare in the rest of the collection. This creates the "peaks" in your 30,000-dimensional sparse vector.</p>
                                                </div>
                                            </details>
                                        </div>
                                    </div>
                                </details>
                            </div>
                        </div>
                    </details>

                    <details class="nested-accordion">
                        <summary>Contextualized Token Embeddings (Late Interaction)</summary>
                        <div class="nested-content">
                            <p>Foundation for fine-grained matching (e.g., BERT, ColBERT). Unlike dense summaries, this generates separate, unique vectors for every token based on its neighbors.</p>
                            <p><strong>Example:</strong> Distinguishing between "bank" in "river bank" vs. "investment bank."</p>
                            <p><strong>When to use:</strong> Best for complex queries where word order and situational meaning change the intent. It allows for "late interaction" to find highly granular matches between specific parts of a query and a document.</p>
                            
                            <div class="tree-root">
                                <details class="nested-accordion">
                                    <summary>How: Contextualized Encoding</summary>
                                    <div class="nested-content">
                                        <div class="tree-root">
                                            <details class="nested-accordion">
                                                <summary>The Practical Workflow</summary>
                                                <div class="nested-content">
                                                    <p><strong>Setup:</strong> involves feeding a sequence into a Transformer-based encoder (e.g., BERT).</p>
                                                    <p><strong>Execution:</strong> self-attention mechanisms compute the relationship between each token and its neighbors, weighting influence based on relevance.</p>
                                                    <p><strong>Output:</strong> a multi-dimensional vector for every token that reflects its precise semantic role within that specific sentence, rather than a generic dictionary definition.</p>
                                                </div>
                                            </details>
                                            
                                            <details class="nested-accordion">
                                                <summary>1. Self-Attention Layers</summary>
                                                <div class="nested-content">
                                                    <p>Dynamically adjusts a token's representation by scanning the entire input, ensuring "bank" shifts based on terms like "river" or "money."</p>
                                                    
                                                </div>
                                            </details>
                                            
                                            <details class="nested-accordion">
                                                <summary>2. Positional Encoding</summary>
                                                <div class="nested-content">
                                                    <p>Infuses word order data into the vectors, allowing the model to distinguish meaning based on structural placement.</p>
                                                    
                                                </div>
                                            </details>
                                            
                                            <details class="nested-accordion">
                                                <summary>3. Late Interaction (MaxSim) 
                                                    <button class="eye-btn" onclick="const viewport = document.querySelector('.retro-viewport'); viewport.querySelector('iframe').src = 'RAG/max_sim.html'; viewport.classList.add('active');">
                                                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye">
                                                            <path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path>
                                                            <circle cx="12" cy="12" r="3"></circle>
                                                        </svg>
                                                    </button>
                                                </summary>
                                                <div class="nested-content">
                                                    <p> Instead of one summary vector, the engine aligns every query token embedding against every document token embedding, calculating the maximum similarity for fine-grained retrieval.</p>
                                                </div>
                                            </details>
                                        </div>
                                    </div>
                                </details>
                            </div>
                        </div>
                    </details>

                    <details class="nested-accordion">
                        <summary>Contrastive Learning Embeddings</summary>
                        <div class="nested-content">
                            <p>Foundation for similarity learning. Pulls similar examples together, pushes dissimilar apart.</p>
                            <p><strong>Example:</strong> Training a model by showing it pairs of "duplicate" customer reviews to pull them together while pushing "unrelated" reviews away.</p>
                            <p><strong>When to use:</strong> Use this during the training phase to refine a model's judgment. It’s the "coach" that teaches the embedding space how to distinguish between subtly different items.</p>
                        </div>
                    </details>

                    <details class="nested-accordion">
                        <summary>Cross-Encoders (Relevance Scoring)</summary>
                        <div class="nested-content">
                            <p>Foundation for deep relevance. Unlike bi-encoders that collapse text into independent "summary" vectors, this architecture processes a query and document pair simultaneously. By concatenating both into a single input sequence, it enables the <strong>Self-Attention</strong> mechanism to perform exhaustive, token-to-token comparisons.</p>
                            <p><strong>Example:</strong> For the query "Is caffeine good for sleep?", the model's attention heads identify specific relationships—detecting if the text claims caffeine helps or hinders rest, rather than just matching keywords.</p>
                            <p><strong>When to use:</strong> Use as a reranker in the final stage of retrieval. Because the O(N²) self-attention over the combined sequence is computationally heavy, it is reserved for the top 50–100 candidates to provide maximum precision.</p>
                            <p><strong>Difference from Contextualized Token Embeddings:</strong>Cross-encoders process the query and document together, allowing for deep, token-level interactions. Bi-encoders map items to separate vectors in a latent space for fast retrieval</p>
                        </div>
                    </details>
                </div>
            </div>
        </details>

        <details class="section">
            <summary>5. RAG: Indexing Methods in a Vector Database</summary>
            <div class="section-content">
                <p><strong>What is a Vector DB:</strong> Traditional databases look for exact word matches, but vector DBs find data that is <em>similar</em> to your query. They organize information as numeric vectors using specialized indexes.</p>
                <div class="tree-root">
                    <details class="nested-accordion">
                        <summary>Hierarchical Navigable Small World (HNSW) </summary>
                        <div class="nested-content">
                            <button class="eye-btn" onclick="let vp = document.querySelector('.retro-viewport'); vp.querySelector('iframe').src='RAG/HNSW.html'; vp.classList.add('active');">
                                <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                                    <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                                </svg>
                            </button>
                            <p>Constructs a multi-layered graph structure. Upper layers contain sparse nodes for rapid "greedy" traversal. Lower layers are denser for fine-grained navigation. Ensures logarithmic search complexity trading high memory usage for speed.</p>
                            <p><strong>When to use:</strong> Use for very large, diverse datasets when low-latency search is the priority and memory (RAM) is not a bottleneck.</p>
                        </div>
                    </details>
                    <details class="nested-accordion">
                        <summary>Locality-Sensitive Hashing (LSH)</summary>
                        <div class="nested-content">
                            <button class="eye-btn" onclick="let vp = document.querySelector('.retro-viewport'); vp.querySelector('iframe').src='RAG/LSH.html'; vp.classList.add('active');">
                                <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                                    <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                                </svg>
                            </button>
                            <p>By dividing the vector space with random boundaries, items that are close together are mathematically likely to end up in the same group, turning a complex search into a simple, fast lookup.</p>
                            <p><strong>When to use:</strong> Ideal for "approximate" matching in massive datasets. Use it when speed and finding "good enough" results (like detecting near-duplicate documents) are more important than 100% precision.</p>
                        </div>
                    </details>
                    <details class="nested-accordion">
                        <summary>Product Quantization (PQ)</summary>
                        <div class="nested-content">
                            <button class="eye-btn" onclick="let vp = document.querySelector('.retro-viewport'); vp.querySelector('iframe').src='RAG/PQ.html'; vp.classList.add('active');">
                                <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                                    <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                                </svg>
                            </button>
                            <p>
                                <strong>Compresses</strong> high-dimensional vectors by breaking them into smaller <strong>sub-parts</strong> and mapping each part to its nearest "representative" value, or <strong>centroid</strong>. Instead of storing massive raw numbers, a <strong>Signature Matrix</strong> stores compact IDs, drastically lowering the <strong>RAM footprint</strong>.
                                <br><br>
                                <strong>When to use:</strong> Focused on <strong>Lossy Compression</strong>. Use this when you need to manage a massive RAM footprint; it reduces the precision of the values rather than the number of dimensions to shrink the index size.
                            </p>
                        </div>
                    </details>
                    <details class="nested-accordion">
                        <summary>Random Projection (RP)</summary>
                        <div class="nested-content">
                            <button class="eye-btn" onclick="let vp = document.querySelector('.retro-viewport'); vp.querySelector('iframe').src='RAG/random_projection.html'; vp.classList.add('active');">
                                <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                                    <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                                </svg>
                            </button>
                            <p><strong>Reduces dimensionality</strong> by multiplying vectors with a fixed random matrix. Maps data to lower-dimensional space while preserving relative distances.</p>
                            <p><strong>When to use:</strong> Focused on <strong>Dimensionality Reduction</strong>. Use this to manage computational complexity; it preserves the "topology" (relative distances) while making data mathematically simpler to handle before you begin the indexing process.</p>
                        </div>
                    </details>
                </div>
            </div>
        </details>

        <details class="section">
            <summary>6. Advanced Retrieval Strategies</summary>
            <div class="section-content">
                <p>This module covers advanced techniques to optimize vector search for either precision (Metadata Filtering) or recall (Expansion & Reranking).</p>
                <details class="nested-accordion">
                    <summary>
                        A. Self-Querying / Filtered Vector Search
                        <button class="eye-btn" onclick="const v=document.querySelector('.retro-viewport'); v.querySelector('iframe').src='RAG/self_query.html'; v.classList.add('active');">
                            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye">
                                <path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path>
                                <circle cx="12" cy="12" r="3"></circle>
                            </svg>
                        </button>
                    </summary>
                    <div class="nested-content">
                        <span class="tag">Focus: Precision</span>
                        <p><em>Best for finding "x" with strict constraints "y".</em></p>
                        <div class="tree-root">
                            <details class="nested-accordion">
                                <summary>PROCESS</summary>
                                <div class="nested-content">
                                    <ol>
                                        <li><strong>Scope and Schema Declaration:</strong> Defines the boundaries. Loads the metadata schema to tell the AI which specific fields exist (Price, Date, Author).</li>
                                        <li><strong>Schema-Based Extraction:</strong> The AI parses a messy query and "filters" it into structured fields (e.g., Date: >2026-01-22).</li>
                                        <li><strong>Entity Resolution:</strong> Maps fuzzy text to unique IDs using a Knowledge Graph, isolating verified records.</li>
                                        <li><strong>Query Enrichment:</strong> Generates a Semantic Vector alongside Metadata Filters.</li>
                                        <li><strong>Filtered Retrieval:</strong> DB applies filters first, then calculates similarity only against compliant documents.</li>
                                    </ol>
                                </div>
                            </details>
                            <details class="nested-accordion">
                                <summary>TECHNICAL BREAKDOWN</summary>
                                <div class="nested-content">
                                    <ol>
                                        <li><strong>Metadata Schema Definition:</strong> Assign structured Metadata Tags during ingestion.</li>
                                        <li><strong>The Self-Query Phase (Intent Parsing):</strong> LLM extracts Semantic Vector: "solar energy" | Filter: <code>{"year": 2024}</code>.</li>
                                        <li><strong>Pre-Filtering Execution:</strong> DB applies filter before calculating similarity to prevent hallsucinations.</li>
                                    </ol>
                                </div>
                            </details>
                        </div>
                    </div>
                </details>
                <details class="nested-accordion">
                    <summary>
                        B. Query Expansion / Reranking
                        <button class="eye-btn" onclick="const viewport = document.querySelector('.retro-viewport'); viewport.querySelector('iframe').src = 'RAG/query_expansion.html'; viewport.classList.add('active');">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path>
                                <circle cx="12" cy="12" r="3"></circle>
                            </svg>
                        </button>
                    </summary>
                    <div class="nested-content">
                        <span class="tag">Focus: Recall</span>
                        <p><em>Best for finding "x" and synonyms.</em></p>
                        <p><strong>Overview:</strong> Increasing retrieval recall by capturing semantically related context that a single vector might overlook.</p>
                        <div class="tree-root">
                            <details class="nested-accordion">
                                <summary>QUERY EXPANSION WORKFLOW (THE HOW)</summary>
                                <div class="nested-content">
                                    <ol>
                                        <li>
                                            <strong>Initiation and Configuration:</strong> 
                                            You configure how many alternate perspectives are required for your specific task. The system validates these parameters and, if in a testing phase, uses "mocking" to simulate the flow.
                                        </li>
                                        <li>
                                            <strong>Persona and Context Injection:</strong> 
                                            A <em>Prompt Template</em> is crafted to define the AI's objective. This is highly flexible: you can instruct the AI to act as a subject matter expert, a technical skeptic, etc. By using placeholders for the persona, original question, and separators, the system adapts the expansion strategy to suit different domains.
                                        </li>
                                        <li>
                                            <strong>Semantic Diversification:</strong> 
                                            The LLM performs the expansion, reframing the original query into diverse linguistic representations. The goal is to traverse different regions of the <strong>embedding space</strong>, ensuring the search isn't trapped by the specific wording or narrow constraints of the initial question.
                                        </li>
                                        <li>
                                            <strong>Structured Parsing:</strong> 
                                            Turns a “wall of text” into a checklist. The LLM outputs all <em>N</em> queries as one long string. The system uses a delimiter (like <code>#next-question#</code>) as a “cutting point” to split that string into a clean list of individual, searchable questions.
                                        </li>
                                        <li>
                                            <strong>High-Recall Retrieval:</strong> 
                                            Casts a “wide net” across the database. The system executes all <em>N</em> queries at once through parallel search. This maximizes recall by covering more embedding space, retrieving context that a single vector might overlook.
                                        </li>
                                    </ol>
                                </div>
                            </details>
                            <details class="nested-accordion">
                                <summary>RERANKING WORKFLOW (PRECISION)</summary>
                                <div class="nested-content">
                                    <p>Reranking transforms a broad list of "potential" matches into a prioritized list of "guaranteed" context.</p>
                                    <ol>
                                        <li>
                                            <strong>Over-Retrieval (The Wide Net):</strong> 
                                            To ensure the best answer isn't missed, the system retrieves more documents than needed (e.g., fetching the top 100 results when the final requirement is only 5).
                                        </li>
                                        <li>
                                            <strong>Cross-Encoder Scoring (Deep Reasoning):</strong> 
                                            The query and each retrieved document are passed into a Cross-Encoder as a pair: <code>[Query][SEP][Document]</code>. Unlike bi-encoders, this allows for <strong>joint attention</strong>, identifying nuanced relationships and specific relevance.
                                        </li>
                                        <li>
                                            <strong>Re-Ordering and Selection:</strong> 
                                            Documents are sorted based on high-precision scores. The most accurate answer—regardless of its initial vector rank—is moved to the top, while the irrelevant "tail" is discarded to keep only the top $K$ results.
                                        </li>
                                    </ol>
                                </div>
                            </details>
                            
                            
                            
                            <details class="nested-accordion">
                                <summary>FILTERED VECTOR SEARCH WORKFLOW (LOGIC)</summary>
                                <div class="nested-content">
                                    <p>Acts as a strict gatekeeper, ensuring the system respects logical boundaries before exploring semantic meanings.</p>
                                    <ul>
                                        <li>
                                            <strong>Metadata Ingestion:</strong> 
                                            During indexing, documents are tagged with structured info (e.g. department, date, clearance). This prevents the database from being "blind" to everything except word similarity.
                                        </li>
                                        <li>
                                            <strong>Query-to-Filter Translation (Self-Querying):</strong> 
                                            The system uses an LLM to extract constraints from the user. Asking for "2023 reports" triggers a mandatory <code>year == 2023</code> metadata filter, converting fuzzy requests into hard technical instructions.
                                        </li>
                                        <li>
                                            <strong>Pre-Filter Execution:</strong> 
                                            The system physically ignores documents that don't match criteria. This makes retrieval faster and immune to "noise" from irrelevant categories.
                                        </li>
                                        <li>
                                            <strong>Similarity Retrieval:</strong> 
                                            Only after narrowing the field does the system perform vector search. This ensures the "best match" is both <strong>semantically relevant</strong> and <strong>logically valid</strong>.
                                        </li>
                                    </ul>
                                </div>
                            </details>
                        </div>
                    </div>
                </details>
            </div>
        </details>

        <details class="section">
            <summary>7. RAG Evaluation</summary>
            <div class="section-content">
                <p>Standard LLM tests check internal knowledge. <strong>RAG evaluation</strong> assesses the whole system: <strong>Retrieval</strong> and <strong>Generation</strong>.</p>
                <div class="tree-root">
                    <details class="nested-accordion">
                        <summary>The RAGAS Framework</summary>
                        <div class="nested-content">
                            <p><strong>RAGAS</strong> uses LLM-as-a-judge to evaluate the relationship between Question, Context, and Answer.</p>
                            <details class="nested-accordion">
                                <summary>1. Faithfulness [EVALUATE LLM]</summary>
                                <div class="nested-content">
                                    <p>Measures <strong>grounding</strong>: Is the answer derived only from the retrieved context?</p>
                                    <p><strong>Step 1:</strong> Extraction of atomic claims.</p>
                                    <p><strong>Step 2:</strong> Verification of each claim against the context.</p>
                                    <p><code>Faithfulness = # verified / total statements</code></p>
                                </div>
                            </details>
                            <details class="nested-accordion">
                                <summary>2. Answer Relevancy [EVALUATE LLM]</summary>
                                <div class="nested-content">
                                    <p>Does the answer address the user intent?</p>
                                    <p>The judge reverse-engineers the question from the answer and calculates cosine similarity with the original query.</p>
                                </div>
                            </details>
                            <details class="nested-accordion">
                                <summary>3. Context Precision [EVALUATE RETRIEVER]</summary>
                                <div class="nested-content">
                                    <p>Analyzes if relevant chunks appear at the top of the retrieval list.</p>
                                </div>
                            </details>
                        </div>
                    </details>
                </div>
            </div>
        </details>

        <details class="section">
            <summary>8. End to End Optimized RAG Inference Flow</summary>
            <div class="section-content">
                <div class="subsection">
                    <span class="subsection-title">1. The Core RAG Synthesis</span>
                    <ul>
                        <li><strong>Retrieval & Context Mapping:</strong> ContextRetriever fetches and maps chunks.</li>
                        <li><strong>The Prompt Factory:</strong> Constructs Augmented Prompt.</li>
                        <li><strong>Inference Execution:</strong> Sends to LLM for grounded answer.</li>
                    </ul>
                </div>
                <div class="subsection">
                    <span class="subsection-title">2. Optimization: Conversation Memory</span>
                    <p>Uses Sliding Window or Summarization Strategy.</p>
                </div>
                <div class="subsection">
                    <span class="subsection-title">3. Optimization: Intelligent Routing</span>
                    <p>Semantic Router directs searches to specific repositories to save cost.</p>
                </div>
                <div class="subsection">
                    <span class="subsection-title">4. Optimization: Hybrid Search</span>
                    <p>Combines Vector Search and BM25 using weighted merging.</p>
                </div>
            </div>
        </details>

</div>
</div>

<div class="retro-viewport">
    <div class="vp-header">
        <span>ARCHIVE_VIEWER_V1.0</span>
        <button class="vp-close" onclick="document.querySelector('.retro-viewport').classList.remove('active'); document.querySelector('.retro-viewport iframe').src='';">X</button>
    </div>
    <div class="vp-body">
        <iframe src=""></iframe>
    </div>
</div>

</body>
</html>