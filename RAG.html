<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Part III: RAG Architectures // Study Guide</title>
<style>
    :root {
        --bg-color: #000000;
        --text-color: #00ff41;
        --accent-color: #00ff41;
        --dim-color: #003b00;
        --border-color: #00ff41;
        --font-main: 'Courier New', Courier, monospace;
        --font-header: 'Arial Black', Impact, sans-serif;
    }

    * { box-sizing: border-box; }

    body {
        margin: 0;
        padding: 0;
        background-color: var(--bg-color);
        color: var(--text-color);
        font-family: var(--font-main);
        line-height: 1.5;
        overflow-x: hidden;
    }

    /* --- VISUALS --- */
    .dither-layer {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        z-index: -1;
        background-image: radial-gradient(circle, #003b00 1px, transparent 1px);
        background-size: 4px 4px;
        opacity: 0.4;
    }

    .scanlines {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        background: linear-gradient(to bottom, rgba(0, 255, 65, 0), rgba(0, 255, 65, 0) 50%, rgba(0, 20, 0, 0.2) 50%, rgba(0, 20, 0, 0.2));
        background-size: 100% 4px;
        pointer-events: none;
        z-index: 9999;
    }

    .container {
        max-width: 900px;
        width: 100%;
        margin: 0 auto;
        padding: 40px 20px;
        border-left: 2px dashed var(--dim-color);
        border-right: 2px dashed var(--dim-color);
        background-color: rgba(0, 10, 0, 0.9);
        min-height: 100vh;
    }

    /* --- TYPOGRAPHY --- */
    h1 {
        font-family: var(--font-header);
        text-transform: uppercase;
        font-size: 2.5rem;
        border-bottom: 5px solid var(--accent-color);
        margin-bottom: 40px;
        color: var(--accent-color);
        text-align: center;
        text-shadow: 0px 0px 8px var(--accent-color);
        word-wrap: break-word;
    }

    h3 { margin-top: 0; color: var(--accent-color); text-transform: uppercase; font-size: 1.1rem; border-bottom: 1px dashed var(--dim-color); padding-bottom: 5px; }
    strong { color: var(--accent-color); text-decoration: none; font-weight: bold; }
    em { font-style: normal; color: #50c878; border-bottom: 1px dotted var(--dim-color); }

    /* --- ACCORDION STYLES --- */
    /* Outer Parts */
    details.part {
        margin-bottom: 30px;
        border: 2px solid var(--border-color);
        background: #000;
        box-shadow: 6px 6px 0px var(--dim-color);
        transition: transform 0.1s;
    }
    details.part[open] { box-shadow: 4px 4px 0px var(--dim-color); transform: translate(2px, 2px); }
    details.part > summary {
        font-family: var(--font-header);
        font-size: 1.5rem;
        padding: 15px 20px;
        background-color: var(--accent-color);
        color: var(--bg-color);
        cursor: pointer;
        list-style: none;
        text-transform: uppercase;
        position: relative;
    }
    details.part > summary::-webkit-details-marker { display: none; }
    details.part > summary::after { content: '+'; position: absolute; right: 20px; font-weight: 900; }
    details.part[open] > summary::after { content: '-'; }
    .part-content { padding: 20px; border-top: 2px solid var(--border-color); }

    /* Inner Sections */
    details.section {
        margin-bottom: 15px;
        border: 1px solid var(--dim-color);
        background: #050505;
    }
    details.section > summary {
        font-family: var(--font-main);
        font-weight: bold;
        padding: 12px;
        background: #0a0a0a;
        color: var(--text-color);
        cursor: pointer;
        list-style: none;
        border-bottom: 1px solid transparent;
        text-transform: uppercase;
        font-size: 1.1rem;
    }
    details.section > summary:hover { background: var(--dim-color); color: var(--accent-color); }
    details.section[open] > summary {
        border-bottom: 1px solid var(--dim-color);
        background: #0f0f0f;
        color: var(--accent-color);
        text-shadow: 0px 0px 5px var(--accent-color);
    }
    .section-content { padding: 20px; }

    /* Nested Subsections (Specifically for Section 6) */
    details.subsection-accordion {
        margin: 15px 0;
        border: 1px solid var(--dim-color);
        background: #020202;
    }
    details.subsection-accordion > summary {
        font-family: var(--font-main);
        font-size: 0.95rem;
        padding: 10px;
        background: #051005;
        color: var(--text-color);
        cursor: pointer;
        list-style: none;
        border-bottom: 1px solid transparent;
    }
    details.subsection-accordion > summary:hover { background: var(--dim-color); }
    details.subsection-accordion[open] > summary {
        background: var(--dim-color);
        color: #fff;
        border-bottom: 1px dashed var(--accent-color);
    }
    details.subsection-accordion[open] > summary::before { content: "[-] "; }
    details.subsection-accordion > summary::before { content: "[+] "; color: var(--accent-color); }

    /* Tags */
    .tag {
        display: inline-block;
        border: 1px solid var(--accent-color);
        color: var(--bg-color);
        background-color: var(--accent-color);
        padding: 2px 8px;
        font-size: 0.75rem;
        font-weight: bold;
        margin-bottom: 15px;
        text-transform: uppercase;
    }

    /* Subsections Generic */
    .subsection {
        margin-bottom: 25px;
        border-left: 4px solid var(--dim-color);
        padding-left: 15px;
    }
    .subsection-title {
        background: var(--dim-color);
        color: var(--accent-color);
        padding: 2px 6px;
        font-weight: bold;
        text-transform: uppercase;
        display: inline-block;
        margin-bottom: 10px;
        font-size: 0.9rem;
    }

    p { margin-bottom: 12px; margin-top: 0; text-align: justify; }
    ul, ol { padding-left: 20px; margin-bottom: 15px; }
    li { margin-bottom: 5px; }

    .code-block {
        background: #020a02;
        border: 1px dashed var(--dim-color);
        padding: 10px;
        margin: 10px 0;
        font-family: 'Courier New', monospace;
        color: var(--accent-color);
        overflow-x: auto;
        white-space: pre-wrap;
    }

    code {
        background-color: var(--dim-color);
        color: #fff;
        padding: 2px 5px;
        font-family: 'Courier New', monospace;
        font-size: 0.9em;
    }

    /* Image Placeholder */
    .img-placeholder {
        border: 1px dashed var(--dim-color);
        background: rgba(0, 59, 0, 0.2);
        color: var(--accent-color);
        padding: 15px;
        text-align: center;
        margin: 15px 0;
        font-size: 0.8rem;
        letter-spacing: 1px;
    }

    /* --- INTERACTIVE: EYE BUTTON & VIEWPORT --- */
    .eye-btn {
        position: relative;
        width: 28px; height: 28px;
        background: #000;
        border: 1px solid var(--accent-color);
        cursor: pointer;
        padding: 4px;
        display: inline-flex;
        align-items: center; justify-content: center;
        margin-left: 8px;
        vertical-align: bottom;
        transition: transform 0.1s;
    }
    .eye-btn svg { width: 100%; height: 100%; fill: var(--accent-color); }
    .eye-btn:hover {
        background: var(--accent-color);
        transform: translate(-1px, -1px);
        box-shadow: 2px 2px 0px var(--dim-color);
    }
    .eye-btn:hover svg { fill: #000; }

    .retro-viewport {
        position: fixed;
        top: 50%; left: 50%;
        transform: translate(-50%, -50%);
        width: 80vw; height: 80vh;
        max-width: 900px; max-height: 700px;
        background-color: #000;
        border: 2px solid var(--accent-color);
        box-shadow: 0 0 50px rgba(0, 50, 0, 0.8);
        display: flex; flex-direction: column;
        z-index: 10000;
        visibility: hidden; opacity: 0;
        pointer-events: none;
        transition: opacity 0.2s;
        resize: both; overflow: hidden;
    }
    .retro-viewport.active { visibility: visible; opacity: 1; pointer-events: auto; }

    .vp-header {
        background: var(--accent-color);
        color: #000;
        padding: 5px 10px;
        font-weight: bold;
        font-family: var(--font-header);
        display: flex; justify-content: space-between;
        align-items: center;
        border-bottom: 2px solid #000;
        cursor: default;
    }
    .vp-close {
        background: #000; color: var(--accent-color);
        border: 1px solid #000; font-weight: 900; 
        cursor: pointer; font-family: var(--font-main);
    }
    .vp-close:hover { background: #fff; color: #000; }
    
    .vp-body { flex-grow: 1; position: relative; background: #000; }
    .vp-body iframe { width: 100%; height: 100%; border: none; }

    /* --- RESPONSIVE --- */
    @media (max-width: 600px) {
        h1 { font-size: 1.8rem; border-bottom-width: 3px; }
        details.part > summary { font-size: 1.1rem; padding: 12px; }
        details.section > summary { font-size: 0.9rem; }
        .container { padding: 10px; border: none; }
        .part-content, .section-content { padding: 10px; }
        p { text-align: left; }
        .retro-viewport { width: 95vw; height: 60vh; }
    }
</style>
</head>
<body>
<div class="dither-layer"></div>
<div class="scanlines"></div>
<div class="container">
<h1>Part III: RAG Architectures</h1>

<div class="part-content">
        
        <details class="section">
            <summary>1. RAG Overview & Modules</summary>
            <div class="section-content">
                <p><strong>RAG:</strong> Pulling new, updated, tailored information from a vector store to concatenate as context to a user prompt, before feeding a foundational model.</p>
                <p><strong>A RAG system is composed of three main modules:</strong></p>
                <ul>
                    <li>Ingestion pipeline: A batch or streaming pipeline used to populate the vector DB</li>
                    <li>Retrieval pipeline: A module that queries the vector DB and retrieves relevant entries to the user’s input</li>
                    <li>Generation pipeline: The layer that uses the retrieved data to augment the prompt and an LLM to generate answers</li>
                </ul>
                
                <div class="subsection">
                    <span class="subsection-title">HOW ARE MODULES CONNECTED OVERVIEW</span>
                    <ol>
                        <li>On the backend side, the ingestion pipeline runs either on a schedule or constantly to populate the vector DB with external data.</li>
                        <li>On the client side, the user asks a question.</li>
                        <li>The question is passed to the retrieval module, which preprocesses the user’s input and queries the vector DB.</li>
                        <li>The generation pipelines use a prompt template, user input, and retrieved context to create the prompt.</li>
                        <li>The prompt is passed to an LLM to generate the answer.</li>
                        <li>The answer is shown to the user.</li>
                    </ol>
                </div>
            </div>
        </details>

        <details class="section">
            <summary>2. RAG Ingestion: Walkthrough</summary>
            <div class="section-content">
                <ol>
                    <li><strong>Pull in the raw data (Data extraction/Ingestion):</strong> Collect documents from databases, PDFs, APIs, etc.</li>
                    <li><strong>Clean it up (Cleaning layer):</strong> Fix messy text: remove weird characters, strip URLs, delete headers/footers.</li>
                    <li><strong>Break into smaller pieces (Chunking/Splitting):</strong> Split large documents into 300-800 word chunks. Smart chunking keeps complete thoughts together.</li>
                    <li><strong>Turn into vectors (Embedding):</strong> Send chunks to an embedding model. Text becomes numerical vectors capturing meaning.</li>
                    <li><strong>Save in vector database (Loading):</strong> Store vectors + metadata (source URL, date, title) in a vector DB.</li>
                </ol>
                <p><strong>Result:</strong> When users ask questions, the system searches vectors for relevant chunks, adds them to the prompt, and the LLM generates accurate, grounded answers.</p>
                <p><strong>In short:</strong> Raw documents &rarr; cleaned &rarr; chunked &rarr; vectorized &rarr; stored &rarr; ready for AI retrieval.</p>
            </div>
        </details>

        <details class="section">
            <summary>3. RAG: Chunking</summary>
            <div class="section-content">
                <p>Chunking is the process of splitting large documents into smaller, discrete segments. In RAG systems, it is essential for:</p>
                <ul>
                    <li><strong>Context Limits:</strong> Ensures text fits within an LLM’s token constraints.</li>
                    <li><strong>Retrieval Precision:</strong> Improves semantic search by focusing on specific topics rather than broad documents.</li>
                    <li><strong>Relevancy:</strong> Helps the model generate accurate answers by providing only the most pertinent context.</li>
                </ul>
                <hr style="border: 0; border-top: 1px dashed var(--dim-color); margin: 20px 0;">
                <ul>
                    <li>
                        <strong>Fixed-size Chunking:</strong><button class="eye-btn" onclick="const v=document.querySelector('.retro-viewport'); v.querySelector('iframe').src='RAG/fixedsize_chunk'; v.classList.add('active');"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg></button> Slices text into hard segments based on a strict token or character limit (e.g., 512 tokens). While computationally efficient, it ignores semantic structure, often breaking sentences mid-thought. To mitigate context loss at the edges, a "sliding window" overlap (10-20%) is applied.
                        <ul>
                            <li><em>Example:</em> Processing a raw server log file. You blindly cut the text every 500 characters, regardless of where the line ends.</li>
                            <li><em>When to use:</em> Ideal for uniform data streams or simple tasks where computational speed is the priority and semantic context is less critical (e.g., n-gram analysis).</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Recursive Chunking:</strong><button class="eye-btn" onclick="const v=document.querySelector('.retro-viewport'); v.querySelector('iframe').src='RAG/recursive_chunk.html'; v.classList.add('active');"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg></button> Iteratively breaks down text using a hierarchy of separators to fit specific size limits. It attempts to split by largest structural elements first (like paragraphs), then sentences, then words.
                        <ul>
                            <li><em>Example:</em> An article is split first by double newlines (paragraphs). If a paragraph is still too big, it splits by periods (sentences).</li>
                            <li><em>When to use:</em> The "default" best choice for general prose, articles, and essays. It respects natural reading flow, keeping related ideas (paragraphs) intact.</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Content-aware / Syntactic Chunking:</strong><button class="eye-btn" onclick="const v=document.querySelector('.retro-viewport'); v.querySelector('iframe').src='RAG/Syntactic_chunker.html'; v.classList.add('active');"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg></button> Leverages the distinct formatting logic of the source file to define boundaries. For code, it splits by functions or classes; for Markdown or HTML, it splits by headers or tags.
                        <ul>
                            <li><em>Example:</em> Splitting a Python script by <code>def</code> blocks or an HTML file by <code>&lt;div&gt;</code> tags, ensuring no function is cut in half.</li>
                            <li><em>When to use:</em> Essential for structured documents like Code, Markdown, or JSON. It prevents breaking syntax, ensuring the retrieved chunk is valid and executable.</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Semantic Chunking:</strong><button class="eye-btn" onclick="const v=document.querySelector('.retro-viewport'); v.querySelector('iframe').src='RAG/semantic_chunker.html'; v.classList.add('active');"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg></button> Semantic chunking splits text into sentences and converts them to embeddings to assess similarity (usually via cosine). Chunks grow as long as sentences stay semantically close; once distance exceeds a threshold, a new chunk starts. This linear process preserves document order and context for accurate retrieval.
                        <ul>
                            <li><em>Example:</em> A transcript moves from discussing "Weather" to "Stocks." The system detects the topic shift via embeddings and cuts the chunk exactly there.</li>
                            <li><em>When to use:</em> Best for RAG systems requiring high precision. It ensures a chunk contains only one distinct concept, reducing noise for the LLM.</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </details>

        <details class="section">
            <summary>4. RAG: Embedding</summary>
            <div class="section-content">
                <p><strong>Dense Embeddings (Bi-Encoders):</strong> Foundation for semantic search. Captures abstract meaning by collapsing an entire sequence into a single "summary" vector.<br>
                <strong>When to use:</strong> Use this when the intent matters more than specific words. It bridges the gap between synonyms and related concepts by mapping them to the same mathematical space, making it the bedrock of modern semantic search, though it may lose granular word-level nuance.</p>
        
                <p><strong>Sparse Embeddings:</strong> Foundation for lexical search. High-dimensional vectors mapping to specific tokens. Essential for exact keyword matching.<br>
                <strong>When to use:</strong> Use this for precision-heavy tasks where exact keyword matching is non-negotiable. While dense models might "hallucinate" a similar concept, sparse embeddings (like BM25) ensure you find the exact string of characters you typed.</p>
        
                <p><strong>Contextualized Token Embeddings (Late Interaction):</strong> Foundation for fine-grained matching (e.g., BERT, ColBERT). Unlike dense summaries, this generates separate, unique vectors for every token based on its neighbors.<br>
                <strong>Example:</strong> Distinguishing between "bank" in "river bank" vs. "investment bank."<br>
                <strong>When to use:</strong> Best for complex queries where word order and situational meaning change the intent. It allows for "late interaction" to find highly granular matches between specific parts of a query and a document.</p>
        
                <p><strong>Contrastive Learning Embeddings:</strong> Foundation for similarity learning. Pulls similar examples together, pushes dissimilar apart.<br>
                <strong>Example:</strong> Training a model by showing it pairs of "duplicate" customer reviews to pull them together while pushing "unrelated" reviews away.<br>
                <strong>When to use:</strong> Use this during the training phase to refine a model's judgment. It’s the "coach" that teaches the embedding space how to distinguish between subtly different items.</p>

                <p><strong>Cross-Encoders (Relevance Scoring):</strong> Foundation for deep relevance. Unlike bi-encoders that collapse text into independent "summary" vectors, this architecture processes a query and document pair simultaneously. By concatenating both into a single input sequence, it enables the <strong>Self-Attention</strong> mechanism to perform exhaustive, token-to-token comparisons.<br>
                <strong>Example:</strong> For the query "Is caffeine good for sleep?", the model's attention heads identify specific relationships—detecting if the text claims caffeine helps or hinders rest, rather than just matching keywords.<br>
                <strong>When to use:</strong> Use as a reranker in the final stage of retrieval. Because the O(N²) self-attention over the combined sequence is computationally heavy, it is reserved for the top 50–100 candidates to provide maximum precision.</p>
            </div>
        </details>

        <details class="section">
            <summary>5. RAG: Indexing Methods in a Vector Database</summary>
            <div class="section-content">
                <p><strong>What is a Vector DB:</strong> Traditional databases look for exact word matches, but vector DBs find data that is <em>similar</em> to your query. They organize information as numeric vectors using specialized indexes.</p>
                
                <div class="subsection">
                    <span class="subsection-title">HNSW (Hierarchical Navigable Small World)</span>
                    <button class="eye-btn" onclick="let vp = document.querySelector('.retro-viewport'); vp.querySelector('iframe').src='RAG/HNSW.html'; vp.classList.add('active');">
                        <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                            <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                        </svg>
                    </button>
                    <p>Constructs a multi-layered graph structure. Upper layers contain sparse nodes for rapid "greedy" traversal. Lower layers are denser for fine-grained navigation. Ensures logarithmic search complexity trading high memory usage for speed.</p>
                    <p><strong>When to use:</strong> Use for very large, diverse datasets when low-latency search is the priority and memory (RAM) is not a bottleneck.</p>
                </div>
        
                <div class="subsection">
                    <span class="subsection-title">LSH (Locality-Sensitive Hashing)</span>
                    <button class="eye-btn" onclick="let vp = document.querySelector('.retro-viewport'); vp.querySelector('iframe').src='RAG/LSH.html'; vp.classList.add('active');">
                        <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                            <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                        </svg>
                    </button>
                    <p>Groups similar vectors into "buckets" by assigning them the same digital fingerprint. By dividing the vector space with random boundaries, items that are close together are mathematically likely to end up in the same group, turning a complex search into a simple, fast lookup.</p>
                    <p><strong>When to use:</strong>Ideal for "approximate" matching in massive datasets. Use it when speed and finding "good enough" results (like detecting near-duplicate documents) are more important than 100% precision.</p>
                </div>
        
                <div class="subsection">
                    <span class="subsection-title">Product Quantization (PQ)</span>
                    <button class="eye-btn" onclick="let vp = document.querySelector('.retro-viewport'); vp.querySelector('iframe').src='RAG/PQ.html'; vp.classList.add('active');">
                        <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                            <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                        </svg>
                    </button>
                    <p>
                        Compresses high-dimensional vectors by breaking them into smaller <strong>sub-parts</strong> and mapping each part to its nearest "representative" value, or <strong>centroid</strong>. Instead of storing massive raw numbers, a <strong>Signature Matrix</strong> stores compact IDs, drastically lowering the <strong>RAM footprint</strong>.
                        <br><br>
                        <strong>When to use:</strong> Focused on <strong>Lossy Compression</strong>. Use this when you need to manage a massive RAM footprint; it reduces the precision of the values rather than the number of dimensions to shrink the index size.
                    </p>
                </div>
        
                <div class="subsection">
                    <span class="subsection-title">Random Projection (RP)</span>
                    <button class="eye-btn" onclick="let vp = document.querySelector('.retro-viewport'); vp.querySelector('iframe').src='RAG/random_projection.html'; vp.classList.add('active');">
                        <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                            <path d="M12 4.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5zM12 17c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5zm0-8c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/>
                        </svg>
                    </button>
                    <p>Reduces dimensionality by multiplying vectors with a fixed random matrix. Maps data to lower-dimensional space while preserving relative distances.</p>
                    <p><strong>When to use:</strong> Focused on <strong>Dimensionality Reduction</strong>. Use this to manage computational complexity; it preserves the "topology" (relative distances) while making data mathematically simpler to handle before you begin the indexing process.</p>
                </div>
            </div>
        </details>

        <details class="section">
            <summary>6. Advanced Retrieval Strategies</summary>
            <div class="section-content">
                <p>This module covers advanced techniques to optimize vector search for either precision (Metadata Filtering) or recall (Expansion & Reranking).</p>
        
                <details class="subsection-accordion">
                    <summary>A. Self-Querying / Filtered Vector Search</summary>
                    <div class="section-content">
                        <span class="tag">Focus: Precision & Security</span>
                        <button class="eye-btn" onclick="const v=document.querySelector('.retro-viewport'); v.querySelector('iframe').src='RAG/self_query.html'; v.classList.add('active');">
                            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-eye"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg>
                        </button>
                        <p><em>Best for finding "x" with strict constraints "y".</em></p>
        
                        <div class="img-placeholder">
                        </div>
                        
                        <h3>Process:</h3>
                        <ol>
                            <li>
                                <strong>Scope and Schema Declaration:</strong> This step defines the boundaries. It loads the metadata schema (the "rulebook") to tell the AI which specific fields exist—like Price, Date, or Author. It sets the stage so the AI knows exactly what categories are valid before processing begins.
                            </li>
                            <li>
                                <strong>Schema-Based Extraction:</strong> This step applies the rules. The AI parses a messy natural language query and "filters" it into the fields defined in Step 1. It transforms "stuff from last week" into Date: >2026-01-22, turning "vibes" into actionable, structured filters.
                            </li>
                            <li>
                                <strong>Entity Resolution:</strong> Entity resolution maps fuzzy text to unique IDs using a relational database or Knowledge Graph. The model identifies an entity and queries an ontology for its specific UUID. This ID acts as a hard metadata filter, forcing the vector search to isolate the verified record and ignore distractors.
                            </li>
                            <li>
                                <strong>Query Enrichment:</strong> This step builds the dual-input command. Instead of merging them, it generates a Semantic Vector (a high-dimensional mathematical representation of the core intent and meaning of the query, stripped of the filter logic) and bundles it alongside the Metadata Filters (the "boundaries"). The database receives two distinct instructions: Search using this vector, but ONLY inside this specific fence.
                            </li>
                            <li>
                                <strong>Filtered Retrieval:</strong> The execution phase. The database applies the Metadata Filters first, masking all non-compliant documents (the "Gate"). The Semantic Vector then calculates similarity only against the remaining "open" candidates, ensuring the search never even "sees" restricted data.
                            </li>
                            <li>
                                <strong>Schema & Metadata:</strong> <strong>The Schema</strong> defines the structure and fields. <strong>Metadata</strong> is the actual data in those fields. Definitions guide the LLM on what to extract.    
                            </li>
                        </ol>
                    
                        <h3>Technical Breakdown:</h3>
                        <ol>
                            <li>
                                <strong>Metadata Schema Definition:</strong> First, you must enrich your raw data. During ingestion, assign structured Metadata Tags (e.g., timestamp, file_type, user_id) to each vector embedding. This creates the "hooks" for future filters.
                            </li>
                            <li>
                                <strong>The Self-Query Phase (Intent Parsing):</strong> When a query arrives, an LLM parses it to extract parameters.
                                <ul>
                                    <li><em>Prompt:</em> "Show me 2024 reports on solar energy."</li>
                                    <li><em>Extraction:</em> Semantic Vector: "solar energy" | Filter: <code>{"year": 2024}</code>.</li>
                                </ul>
                            </li>
                            <li>
                                <strong>Pre-Filtering Execution:</strong> The vector database applies the filter <em>before</em> calculating similarity. It masks all vectors that don't match year: 2024, drastically reducing the search space and preventing "semantic hallucinations" from other years.
                            </li>
                            <li>
                                <strong>Vector Similarity Search:</strong> The engine calculates the distance (e.g., Cosine Similarity) between the query vector and only the "surviving" candidates.
                            </li>
                        </ol>
                    </div>
                </details>
        
                <details class="subsection-accordion">
                    <summary>B. Query Expansion & Reranking</summary>
                    <div class="section-content">
                        <span class="tag">Focus: Recall</span>
                        <p><em>Best for finding "x" and synonyms.</em></p>
                        
                        <p><strong>Overview:</strong> Increasing retrieval recall by capturing semantically related context that a single vector might overlook. </p>
                        
                        <div class="img-placeholder">
                                                    </div>
    
                        <p><em>Warning: Although query expansion is useful, it can introduce latency as a consequence of searching for multiple vectors instead of one; to mitigate this, you can introduce parallelization methods.</em></p>
                    
                        <h3>Query Expansion Workflow (The How):</h3>
                        <ol>
                            <li>
                                <strong>Initiation and Configuration:</strong> The process begins by defining the scope of expansion. You configure how many alternate perspectives N are required for your specific task. 
                            </li>
                            <li>
                                <strong>Persona and Context Injection:</strong> Placeholders modularize expansion. The {persona} forces the AI to pivot its worldview, generating diverse sub-queries by viewing the {query} through specialized filters. The {separator} provides a hard boundary, ensuring the system can programmatically slice and ingest these distinct "shards" of reasoning without data bleed or cross-contamination.
                            </li>
                            <li>
                                <strong>Semantic Diversification:</strong> The LLM performs the expansion, reframing the original query into diverse linguistic representations. This step is the "engine" of flexibility; the model adjusts for jargon, synonyms, or conceptual overlaps.
                            </li>
                            <li>
                                <strong>Structured Parsing:</strong> Structured Parsing uses {persona} and {query} to generate expansion variety, while the {separator} acts as a reliable "cutting point." This converts a "wall of text" into a clean, programmatic checklist of N individual search terms.
                            </li>
                            <li>
                                <strong>High-Recall Retrieval:</strong> Casts a "wide net" across the database. The system executes all N queries at once through parallel search. This maximizes recall by covering more embedding space, retrieving context that a single vector might overlook.
                            </li>
                        </ol>
                    
                        <h3>Reranking Workflow:</h3>
                        <p>Reranking transforms a broad list of "potential" matches into a prioritized list of "guaranteed" context.</p>
                        <ul>
                            <li>
                                <strong>Over-Retrieval (The Wide Net):</strong> Initial vector search is fast but "shallow." To ensure we don't miss the best answer, we retrieve more documents than we actually need (e.g., fetching 100 results when we only want to show 5). This compensates for the "semantic noise" where the database might rank a popular but irrelevant document higher than a niche, perfect match.
                            </li>
                            <li>
                                <strong>Cross-Encoder Scoring (Deep Reasoning):</strong> The system feeds [Query][SEPATOR][DOCUMENT] as a single pair. This enables joint attention, allowing the model to weigh [QUERY]] tokens against [DOCUMENT] tokens simultaneously. It outputs a precision score that reflects true semantic relevance.
                            </li>
                            <li>
                                <strong>Re-Ordering and Selection:</strong> Based on the Cross-Encoder’s high-precision scores, results are re-sorted, ignoring initial vector ranks. This "promotes" niche, highly relevant documents buried in the wide net. A final top-K cutoff discards the noise, leaving only the most potent context.
                            </li>
                        </ul>
                    </div>
                </details>
        
                <details class="subsection-accordion">
                    <summary>C. Strategic Application Matrix</summary>
                    <div class="section-content">
                        
                        <div class="img-placeholder">
                                                    </div>
                        
                        <p><strong>1. Query Expansion</strong></p>
                        <ul>
                            <li><strong>Use Case:</strong> Identify "x" and everything synonymous with "x" without regard for rigid constraints.</li>
                            <li><strong>Goal:</strong> <strong>Recall</strong>.</li>
                            <li><strong>Context:</strong> Appropriate when ensuring that no relevant document is missed simply because the author used different terminology. It prioritizes a broad semantic net over strict boundaries.</li>
                        </ul>
                    
                        <p><strong>2. Self-Querying</strong></p>
                        <ul>
                            <li><strong>Use Case:</strong> Find "x" while strictly adhering to constraints "y".</li>
                            <li><strong>Goal:</strong> <strong>Precision & Security</strong>.</li>
                            <li><strong>Context:</strong> Appropriate when precision is the priority. By converting natural language into hard filters, it guarantees the AI only searches within a legally or operationally defined "safe zone," preventing the retrieval of irrelevant or unauthorized data.</li>
                        </ul>
                    
                        <p><strong>3. Hybrid Approach</strong></p>
                        <ul>
                            <li><strong>Use Case:</strong> Identify "x" and its synonyms while adhering to constraints "y".</li>
                            <li><strong>Goal:</strong> <strong>High-stakes retrieval</strong>.</li>
                            <li><strong>Context:</strong> Appropriate when you cannot afford to miss information due to phrasing but must also respect strict logical or security boundaries. It combines the broad reach of expansion with the surgical precision of filtering.</li>
                        </ul>
                    </div>
                </details>
            </div>
        </details>

        <details class="section">
            <summary>7. RAG Evaluation</summary>
            <div class="section-content">
                <p>Standard LLM tests check internal knowledge. <strong>RAG evaluation</strong> assesses the whole system: <strong>Retrieval</strong> (finding facts) and <strong>Generation</strong> (using them). It measures Faithfulness, Answer Relevance, and Context Relevance to ensure the output is grounded in the source.</p>
        
                <details class="subsection">
                    <summary>The RAGAS Framework</summary>
                    <div class="subsection-content">
                        <p><strong>RAGAS (Retrieval-Augmented Generation Assessment)</strong> is a framework used to quantify the performance of your RAG pipeline. Ragas uses <strong>LLM-as-a-judge</strong> to evaluate the relationship between the <strong>Question</strong>, the <strong>Context</strong> (retrieved facts), and the <strong>Answer</strong>.</p>
                        
                        <details class="subsection">
                            <summary>1. Faithfulness [EVALUATE LLM -> HALLUCINATION METRIC]</summary>
                            <div class="subsection-content">
                                <p><strong>Core Concept:</strong> It strictly measures <strong>grounding</strong>: the extent to which the generated answer is derived <em>only</em> from the retrieved context, ignoring the model's pre-trained world knowledge.</p>
                                
                                <p><strong>The "Judge" Process:</strong> RAGAS employs a two-step prompting strategy:</p>
                                
                                <div class="process-step">
                                    <p><strong>1. Statement Extraction:</strong> The "Judge" LLM acts as a deconstructor. It takes a complex, multi-sentence response and strips it down into a list of atomic claims—single, verifiable facts. By isolating these individual threads, the model prevents a single correct statement from "masking" a hallucination hidden within the same sentence, ensuring every specific piece of information is exposed for scrutiny.</p>
                                    <blockquote>
                                        <strong>Example:</strong><br>
                                        <em>Generated Answer:</em> "The sky is blue because of Rayleigh scattering and it's 70°F."<br>
                                        <em>Atomic Claims:</em>
                                        <ul>
                                            <li>(A) "The sky is blue."</li>
                                            <li>(B) "The reason is Rayleigh scattering."</li>
                                            <li>(C) "The temperature is 70°F."</li>
                                        </ul>
                                    </blockquote>
                                </div>
                        
                                <div class="process-step">
                                    <p><strong>2. Verification:</strong> The LLM now acts as a binary auditor. It takes each atomic claim and holds it up against the provided Context (and only the context). It asks: "Is this specific fact supported by the text?" If the context explicitly confirms the claim, it receives a 1 (True); if the context is silent or contradicts it, it receives a 0 (False).</p>
                                    <blockquote>
                                        <strong>Example:</strong><br>
                                        <em>Retrieved Context:</em> "Atmospheric scattering causes a blue hue."<br>
                                        <em>Audit Results:</em>
                                        <ul>
                                            <li>(A) Verified (Atmospheric scattering/blue hue).</li>
                                            <li>(B) Verified (Rayleigh is the specific scattering type implied).</li>
                                            <li>(C) Hallucination (The context does not mention temperature).</li>
                                        </ul>
                                    </blockquote>
                                </div>
                        
                                <p><strong>The Calculation:</strong> <code>Faithfulness = # of verified statements / total number of statements</code></p>
                                <p><em>Calculation for example: 2 verified / 3 total = 0.66 Faithfulness.</em></p>
                                
                                <p><strong>Why It Matters:</strong> Critical for enterprise applications (like legal or medical bots). If the context doesn't say it, the model shouldn't say it, even if it is true in the real world.</p>
                            </div>
                        </details>
        
                        <details class="subsection">
                            <summary>2. Answer Relevancy [EVALUATE LLM]</summary>
                            <div class="subsection-content">
                                <p><strong>Core Concept:</strong> To measure Answer Relevancy, the "Judge" uses a Reverse Engineering strategy to see if the answer actually addresses the user's intent:</p>
                                <p><strong>The "Judge" Process (Reverse Engineering):</strong></p>
                                <ul>
                                    <li><strong>Question Generation:</strong> The LLM acts as an inverted strategist. It ignores the original user query and examines only the generated answer, attempting to reverse-engineer the intent. It generates several potential questions that would logically lead to that specific answer, revealing what the model actually addressed.</li>
                                    <li><strong>Embedding Similarity:</strong> RAGAS converts the Original Query and the Generated Questions into high-dimensional vector embeddings. It then calculates the Cosine Similarity (the angle) between them.</li>
                                </ul>
                                <p><strong>The Calculation:</strong> The final score is the mean cosine similarity of all generated questions. A score near 1.0 indicates the answer is a direct hit, while a low score suggests the model is "hedging" or providing a generic summary that doesn't target the prompt.</p>                                
                            </div>
                        </details>
        
                        <details class="subsection">
                            <summary>3. Context Precision [EVALUATE RETRIEVER]</summary>
                            <div class="subsection-content">
                                <p><strong>Core Concept:</strong> In Context Precision, the "chunks" being measured are the specific text segments returned by your retriever for a given query.</p>
                                <p><strong>The "Judge" Process:</strong></p>
                                <ul>
                                    <li><strong>Verification:</strong> The LLM compares retrieved chunks against Ground Truth to classify them as Relevant or Irrelevant.</li>
                                    <li><strong>Ranking Check:</strong> It analyzes the list order, checking if "Relevant" chunks appear before "Irrelevant" ones.</li>
                                </ul>
                                <p><strong>The Calculation:</strong> Uses <strong>Average Precision</strong>, calculating the mean precision score at every rank position where a relevant chunk is found.</p>
                            </div>
                        </details>
                    </div>
                </details>
        
                <details class="subsection">
                    <summary>HOW: The Evaluation Workflow</summary>
                    <div class="subsection-content">
                        <ol>
                            <li><strong>Data Preparation:</strong> Organize RAG outputs into a dataset containing: Question, Contexts, Answer, and Ground Truth (optional).</li>
                            <li><strong>The Evaluation Loop:</strong> Initialize an Evaluator LLM (e.g., GPT-4o) and call the <code>evaluate()</code> function.</li>
                            <li><strong>Under the Hood:</strong> The judge performs specific linguistic tasks: <em>Extraction</em> (for Faithfulness), <em>Verification</em> (logic checks), and <em>Back-Projection</em> (for Answer Relevancy).</li>
                            <li><strong>Output:</strong> A dataframe or dictionary with scores (0 to 1) identifying exactly where the system is failing—whether it's the search (retriever) or the writing (generator).</li>
                        </ol>
                    </div>
                </details>
            </div>
        </details>

        <details class="section">
            <summary>8. End to End Optimized RAG Inference Flow</summary>
            <div class="section-content">
                <div class="subsection">
                    <span class="subsection-title">1. The Core RAG Synthesis</span>
                    <ul>
                        <li><strong>Retrieval & Context Mapping:</strong> ContextRetriever fetches and maps chunks.</li>
                        <li><strong>The Prompt Factory:</strong> Constructs "Augmented Prompt" with template.</li>
                        <li><strong>Inference Execution:</strong> Sends to LLM for grounded answer.</li>
                    </ul>
                </div>
                <div class="subsection">
                    <span class="subsection-title">2. Optimization: Conversation Memory</span>
                    <p>Uses Sliding Window or Summarization Strategy to handle stateless LLMs.</p>
                </div>
                <div class="subsection">
                    <span class="subsection-title">3. Optimization: Intelligent Routing</span>
                    <p>Semantic Router directs searches to specific repositories (e.g., Code vs Articles) to save cost.</p>
                </div>
                <div class="subsection">
                    <span class="subsection-title">4. Optimization: Hybrid Search</span>
                    <p>Combines Vector Search and BM25 using score normalization and weighted merging.</p>
                </div>
                <div class="subsection">
                    <span class="subsection-title">5. Optimization: Multi-Index Vectors</span>
                    <p>Embeds metadata (Platform, Date) into the vector index to weigh authority/recency.</p>
                </div>
            </div>
        </details>

</div>
</div>

<div class="retro-viewport">
    <div class="vp-header">
        <span>ARCHIVE_VIEWER_V1.0</span>
        <button class="vp-close" onclick="document.querySelector('.retro-viewport').classList.remove('active'); document.querySelector('.retro-viewport iframe').src='';">X</button>
    </div>
    <div class="vp-body">
        <iframe src=""></iframe>
    </div>
</div>

</body>
</html>