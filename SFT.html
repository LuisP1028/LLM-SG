
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Part IV: Supervised Fine-Tuning // Study Guide</title>
<style>
/* --- STYLE CONFIGURATION --- (Identical to Main Shell) */
:root {
--bg-color: #000000;
--text-color: #00ff41;
--accent-color: #00ff41;
--dim-color: #003b00;
--border-color: #00ff41;
--font-main: 'Courier New', Courier, monospace;
--font-header: 'Arial Black', Impact, sans-serif;
}
code
Code
* { box-sizing: border-box; }

    body {
        margin: 0;
        padding: 0;
        background-color: var(--bg-color);
        color: var(--text-color);
        font-family: var(--font-main);
        line-height: 1.5;
        overflow-x: hidden;
    }

    .dither-layer {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        z-index: -1;
        background-image: radial-gradient(circle, #003b00 1px, transparent 1px);
        background-size: 4px 4px;
        opacity: 0.4;
    }

    .scanlines {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        background: linear-gradient(to bottom, rgba(0, 255, 65, 0), rgba(0, 255, 65, 0) 50%, rgba(0, 20, 0, 0.2) 50%, rgba(0, 20, 0, 0.2));
        background-size: 100% 4px;
        pointer-events: none;
        z-index: 9999;
    }

    .container {
        max-width: 900px;
        width: 100%;
        margin: 0 auto;
        padding: 40px 20px;
        border-left: 2px dashed var(--dim-color);
        border-right: 2px dashed var(--dim-color);
        background-color: rgba(0, 10, 0, 0.9);
        min-height: 100vh;
    }

    h1 {
        font-family: var(--font-header);
        text-transform: uppercase;
        font-size: 2.5rem;
        border-bottom: 5px solid var(--accent-color);
        margin-bottom: 40px;
        color: var(--accent-color);
        text-align: center;
    }

    strong { color: var(--accent-color); text-decoration: underline; }
    em { font-style: normal; color: #50c878; border-bottom: 1px dotted var(--dim-color); }

    /* ACCORDION STYLES */
    details.section {
        margin-bottom: 15px;
        border: 1px solid var(--dim-color);
        background: #050505;
    }

    details.section > summary {
        font-family: var(--font-main);
        font-weight: bold;
        padding: 12px;
        background: #0a0a0a;
        color: var(--text-color);
        cursor: pointer;
        list-style: none;
        border-bottom: 1px solid transparent;
        text-transform: uppercase;
        font-size: 1.1rem;
    }

    details.section > summary:hover { background: var(--dim-color); color: var(--accent-color); }
    details.section[open] > summary {
        border-bottom: 1px solid var(--dim-color);
        background: #0f0f0f;
        color: var(--accent-color);
        text-shadow: 0px 0px 5px var(--accent-color);
    }

    .section-content { padding: 20px; }

    .subsection {
        margin-bottom: 25px;
        border-left: 4px solid var(--dim-color);
        padding-left: 15px;
    }

    .subsection-title {
        background: var(--dim-color);
        color: var(--accent-color);
        padding: 2px 6px;
        font-weight: bold;
        text-transform: uppercase;
        display: inline-block;
        margin-bottom: 10px;
        font-size: 0.9rem;
    }

    p { margin-bottom: 12px; margin-top: 0; text-align: justify; }
    ul { padding-left: 20px; margin-bottom: 15px; }
    li { margin-bottom: 5px; }

    .code-block {
        background: #020a02;
        border: 1px dashed var(--dim-color);
        padding: 10px;
        margin: 10px 0;
        font-family: 'Courier New', monospace;
        color: var(--accent-color);
        overflow-x: auto;
        white-space: pre-wrap;
    }
</style>
</head>
<body>
<div class="dither-layer"></div>
<div class="scanlines"></div>
<div class="container">
<h1>Part IV: Supervised Fine-Tuning</h1>
code
Code
<div class="part-content">
    
    <details class="section">
        <summary>1. Preparations for SFT & Data Curation</summary>
        <div class="section-content">
            <p>Refines the model’s capabilities using carefully curated pairs of instructions and corresponding answers.</p>
            
            <div class="subsection">
                <span class="subsection-title">Essential Dimensions</span>
                <ul>
                    <li><strong>Accuracy:</strong> Accuracy ensures that the data is factually correct and directly answers the user's specific instruction. It is crucial for building trust, as the model must learn to provide reliable, true information rather than incorrect facts or irrelevant responses.</li>
                    <li><strong>Diversity:</strong> Diversity means the dataset must cover a wide variety of topics, writing styles, and lengths. It should represent the many different ways people will actually use the model. This variety helps the AI become flexible enough to handle new situations and valid requests.</li>
                    <li><strong>Complexity:</strong> Complexity avoids easy or trivial examples that do not challenge the model. Instead, the data should include difficult tasks and multi-step reasoning problems. Training on harder material pushes the AI to develop the logic needed to solve real-world problems.</li>
                    <li><strong>Data Quantity:</strong> The Hugging Face Hub contains numerous instruction datasets, which can be general-purpose or designed for particular tasks or domains.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Task vs. Domain Specific Models</span>
                <p><strong>Task-specific</strong> models focus on performing a single action, such as translation or summarization, often requiring less data and smaller model sizes. For task specific models, data curation often involves collecting examples of the desired task from existing datasets or creating new ones.</p>
                <p><strong>Domain-specific</strong> models, however, focus on mastering a specific industry, like law or medicine. They prioritize specialized knowledge and vocabulary, with data needs varying based on the field's complexity. Domains that are well-represented in the original training data may require less fine-tuning, while those that are more specialized or underrepresented may need more extensive datasets.</p>
            </div>

            <div class="subsection">
                <span class="subsection-title">Data Curation</span>
                <p><strong>Rule-based filtering</strong> is a quality control method that uses explicit, pre-set rules to automatically keep or remove data samples. It aims to maintain high standards by strictly enforcing specific criteria. Its speed and efficiency allow for rapid application to large volumes of data, making it highly scalable.</p>
                <ul>
                    <li><strong>Length filtering:</strong> This rule manages data quality by setting minimum and maximum size limits. It removes responses that are too short to be meaningful or too long and repetitive, though the specific limits depend on the task (e.g., a summary vs. a detailed explanation).</li>
                    <li><strong>Keyword exclusion:</strong> This rule filters data based on specific content. It scans for and removes samples containing banned words—such as profanity, spam, or slang—to ensure the data matches the intended tone and creates a safe environment.</li>
                    <li><strong>Format checking:</strong> This rule validates the structure of the data. It ensures that samples, particularly technical ones like computer code or JSON, follow specific formatting requirements so they remain consistent and usable.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Data Deduplication</span>
                <p><strong>Exact Deduplication:</strong> Exact deduplication removes identical entries by normalizing data and comparing unique hash codes (e.g., SHA-256). If hashes match, data is duplicated. While precise, this method only catches exact copies and misses slightly modified or semantically similar content.</p>
                <p><strong>Fuzzy Deduplication:</strong> Fuzzy deduplication detects near-duplicates using MinHash. It breaks data into shingle fingerprints and checks Jaccard similarity to efficiently spot matches.  Shingles are overlapping subsequences (often n-grams of words or characters) used to convert a text document into a mathematical set. For example, "red car" might become the set {"red", "ed ", " ca", "car"}. Jaccard similarity measures the resemblance between two such sets. It is calculated by dividing the size of the intersection (shared shingles) by the size of the union (total unique shingles). A score of 1.0 indicates identical sets, while 0.0 means no overlap.</p>
            </div>

            <div class="subsection">
                <span class="subsection-title">Data Decontamination</span>
                <p>Applies the tools from data deduplication with a specific boundary: it ensures no material from the test/evaluation set exists within the training set. Adapting these methods changes the target: compare training data against the test set to find overlaps.</p>
                <ul>
                    <li><strong>Exact:</strong> Compute hashes (e.g., SHA-256) for both sets. If a training hash matches a test hash, delete the training sample.</li>
                    <li><strong>Fuzzy:</strong> Use MinHash or N-grams to find training samples similar to test samples. If similarity exceeds a threshold, remove the training data to prevent leakage.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Data Quality Evaluation</span>
                <p>Data quality evaluation measures attributes like accuracy, diversity, and complexity to ensure datasets are fit for training models. To avoid the high cost of human annotation, developers increasingly use LLM-as-a-judge workflows to automate this assessment.</p>
                <p><strong>LLM-as-a-judge</strong> strategy involves prompting LLMs to evaluate the quality of each sample:</p>
                <ul>
                    <li><strong>Domain Specificity:</strong> For specialized fields (e.g., medicine), a model trained on that niche often evaluates technical context better than a "smarter," general-purpose model.</li>
                    <li><strong>Scoring Method:</strong> Comparative assessment (ranking "A is better than B") is more reliable than absolute scoring (rating 1–5). Models find it easier to distinguish relative quality than to adhere to an arbitrary numeric standard.</li>
                </ul>
                
                <span class="subsection-title">Biases of LLM-as-a-judge</span>
                <ul>
                     <li><strong>Position Bias:</strong> Favoring the first answer presented.</li>
                     <li><strong>Verbosity Bias:</strong> Preferring longer responses.</li>
                     <li><strong>Self-Preference:</strong> Rating their own model family higher.</li>
                </ul>
                <p>Counter these by randomizing order, length normalization, and using diverse judges.</p>

                <p><strong>Reward Models:</strong> Choose a reward model when evaluating massive datasets where inference latency and cost are critical constraints. A reward model is structurally modified: it replaces the text-generating layer with a linear head to directly output a raw numerical scalar score. One narrates quality; the other calculates it. LLM-as-a-judge uses standard text generation; it is prompted to "speak" the result (e.g., generating the token "5"). Use LLM-as-a-judge for explainability or zero-shot tasks; use Reward Models for ranking pipelines (like RLHF) that need a stable, continuous signal.</p>
                <p><strong>Encoder Only Models:</strong> Encoder-only models (like BERT) are optimized to convert text into embeddings, making them ideal for rapid classification.</p>
            </div>

            <div class="subsection">
                <span class="subsection-title">Data Exploration & Generation</span>
                <p><strong>Data Exploration:</strong> Statistical analysis uses NLP libraries like NLTK and spaCy to scan text for vocabulary diversity and bias via tokenization. Tools like Matplotlib and Seaborn visualize this data. Topic clustering automatically groups similar documents to uncover hidden themes and trends within large datasets. It converts text into numerical embeddings using sentence transformers. Algorithms like DBSCAN then group these vectors based on their proximity in this mathematical space.</p>
                <p><strong>Data Generation:</strong> When the available instruction datasets are not sufficient, creating custom data becomes necessary. Synthetic data pipelines generate and validate training sets. They tune attributes to fill gaps but need oversight to stop inherited errors and ensure diversity.</p>
                <p>Data generation builds custom datasets starting with a <strong>taxonomy</strong>. Taxonomy refers to both the organized map of skills—breaking a broad domain into specific sub-capabilities to ensure full coverage—and the specific seed instructions used to prompt the model. Taxonomy is the <em>what</em> (the list of skills/topics). Seed is the <em>how</em> (the prompt template used to generate the data).</p>
            </div>

            <div class="subsection">
                <span class="subsection-title">Data Augmentation</span>
                <p>In this context, data augmentation refers to the process of increasing both the quantity and the quality of data samples. Generation creates data from scratch. Augmentation takes actual, existing samples and transforms them (e.g., rewording) to add variety or fix errors.</p>
                <p><strong>Evol-Instruct:</strong></p>
                <ul>
                    <li><strong>In-Depth Evolving (Making it Harder):</strong> This increases the difficulty of a specific prompt to force the model to think harder. Techniques include Constraints, Deepening, Concretizing , Reasoning Steps, and Complicating Input.</li>
                    <li><strong>In-Breadth Evolving (Making it Diverse):</strong> This increases variety. It generates completely new, unique instructions inspired by the original ones, focusing on rare or niche topics to prevent the dataset from being repetitive.</li>
                </ul>
                <p><strong>UltraFeedback:</strong> Evol-Instruct upgrades the Question (Input). UltraFeedback upgrades the Answer (Output). It keeps the prompt the same but generates multiple responses from different models. An LLM AS A JUDGE then rates these responses, adhering to your constraints, to select the highest-quality answer for training. One builds better prompts; the other curates better answers.</p>
                <p><strong>Example:</strong> The Inputs: You generate 4 different answers for one question using different models. The Rubric: You give a "Judge" LLM a specific scorecard. The Selection: The Judge reads the text, scores it based on that rubric, and keeps only the highest-scoring answer.</p>
            </div>
        </div>
    </details>

    <details class="section">
        <summary>2. Evol Instruct Examples</summary>
        <div class="section-content">
            <div class="subsection">
                <span class="subsection-title">Infrastructure</span>
                <ul>
                    <li><strong>Base Instruction:</strong> Inspect the bridge for damage.</li>
                    <li><strong>In-Depth (Constraint):</strong> Inspect the bridge for micro-fractures in the support pylons using thermal imaging data, citing specific ISO safety standards for load-bearing capacity.</li>
                    <li><strong>In-Breadth (New Topic):</strong> Develop a predictive maintenance schedule for an aging underground sewer network based on soil acidity levels.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Defense</span>
                <ul>
                    <li><strong>Base Instruction:</strong> Analyze satellite imagery.</li>
                    <li><strong>In-Depth (Complicating Input):</strong> Analyze the provided raw JSON telemetry from a surveillance drone to identify camouflaged vehicle heat signatures, ignoring background radiation noise.</li>
                    <li><strong>In-Breadth (New Topic):</strong> Formulate a jamming protocol to disrupt enemy communication channels during a high-altitude paratrooper insertion.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Healthcare</span>
                <ul>
                    <li><strong>Base Instruction:</strong> Diagnose the patient's fever.</li>
                    <li><strong>In-Depth (Reasoning):</strong> Diagnose the fever in a post-transplant patient exhibiting rejection signs, differentiating it from a bacterial infection using the provided blood panel results.</li>
                    <li><strong>In-Breadth (New Topic):</strong> Outline a triage procedure for a mass-casualty event involving exposure to a rare neurotoxin in a metropolitan area.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Aerospace</span>
                <ul>
                    <li><strong>Base Instruction:</strong> Check the landing gear.</li>
                    <li><strong>In-Depth (Constraint):</strong> Troubleshoot the landing gear hydraulic failure occurring specifically at -40°C, proposing a fix that adds no more than 5kg of weight to the assembly.</li>
                    <li><strong>In-Breadth (New Topic):</strong> Calculate the trajectory correction burn required for a probe entering a polar orbit around Europa.</li>
                </ul>
            </div>
        </div>
    </details>

    <details class="section">
        <summary>3. Creating an Instruction Dataset</summary>
        <div class="section-content">
            
            <div class="subsection">
                <span class="subsection-title">Instruction Dataset Formats</span>
                <p>Instruction datasets organize text for AI training.</p>
                <ul>
                    <li><strong>Alpaca:</strong> format is designed for single-turn tasks (one prompt, one reply).</li>
                    <li><strong>ShareGPT or OpenAI:</strong> for multi-turn conversations are superior because they track dialogue history using lists of messages.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Chat Templates</span>
                <p>Chat templates are formatting schemas that use special tokens to structure dialogue so the model understands who is speaking (e.g., User vs. Assistant). Their usage depends on the model type:</p>
                <ul>
                    <li><strong>Base Model:</strong> A raw model trained only to predict the next word in a sequence. It has no inherent conversation structure, so you can choose any template (like ChatML) when preparing it for training.</li>
                    <li><strong>Instruct Model:</strong> A base model that has been "fine-tuned" to follow instructions. This training process bakes in a specific template. You must reuse that exact template, or the model will lose the context of the conversation.</li>
                </ul>
                <p>Chat templates rigorously structure text into three roles: system, user, and assistant.</p>
            </div>
        </div>
    </details>

    <details class="section">
        <summary>4. Fine Tuning Methods (LoRA & QLoRA)</summary>
        <div class="section-content">
            
            <div class="subsection">
                <span class="subsection-title">LoRA - Low Rank Adaptation</span>
                <p>Instead of retraining a model’s entire massive web of parameters—which requires immense computational power—LoRA keeps the original pre-trained weights ($W$) frozen. </p>
                
                <p><strong>1. Base Model Preparation:</strong> Load your pre-trained model (the "Base Model") in a quantized format (e.g., 4-bit) to save memory. At this stage, the model is standard. You must "freeze" the model, ensuring no original parameters will be updated during training. The PEFT library handles this freezing automatically when you apply the configuration.</p>
                
                <p><strong>2. Configuration (The Ontology):</strong> You must define a LoraConfig object to dictate how the adaptation works.</p>
                <ul>
                    <li><strong>r (Rank):</strong> Set the dimension of the low-rank matrices ($A$ and $B$). Start with 8 or 16. Higher ranks (e.g., 64) allow for more complex learning but increase VRAM usage.</li>
                    <li><strong>lora_alpha (Alpha):</strong> Set the scaling factor. A standard heuristic is $\alpha = 2 \times r$ (e.g., if rank is 16, set alpha to 32). This ensures the update is strong enough to influence the model.</li>
                    <li><strong>target_modules:</strong> List the specific layers to attach LoRA to. For Style/Format: Target ["q_proj", "v_proj"] (Attention Query/Value). For Reasoning/Facts: Target ["gate_proj", "up_proj", "down_proj"] (MLP layers) in addition to attention.</li>
                    <li>Matrix $A$ compresses the input down to the small rank (r). Matrix $B$ expands it back up to the original size.</li>
                </ul>
                
                <p><strong>3. Model Injection:</strong> Apply the configuration to create the parallel adapter structure. Result: This attaches small, trainable matrices ($A$ and $B$) to the frozen layers. Initially representing zero change, $A$ and $B$ are updated during backpropagation to learn the delta ($\Delta W$)—the precise mathematical offset required to steer the model's output. Because $A$ and $B$ are structurally small (low-rank), they capture this task-specific adjustment using only a fraction of the total parameters.</p>
                
                <p><strong>4. Training (Fine-Tuning):</strong> Run your standard training loop (e.g., using Hugging Face Trainer). During backpropagation, the optimizer ignores the frozen base weights and only updates the specific LoRA matrices you injected. This prevents "catastrophic forgetting" of the base model's knowledge.</p>
                
                <p><strong>5. Merging & Inference:</strong> Save only the Adapter (the LoRA weights), which is small (usually <100MB). To use the model, load the Base Model again and strictly "merge" the saved Adapter weights into it. The math is: $W_{final} = W_{base} + (B \times A \times \alpha/r)$. We add the results together because LoRA is an additive modification rather than a replacement. In the Transformer architecture, the original weights ($W$) represent the model's vast, general knowledge. When you fine-tune for a specific task, you don't want to discard that foundation; you want to nudge it.</p>
            </div>

            <div class="subsection">
                <span class="subsection-title">QLoRA</span>
                <p>Think of QLoRA as LoRA with a world-class packing strategy. It maintains the same low-rank logic but adds layers of compression to make massive models fit on modest hardware.</p>
                <p><strong>Quantization</strong> is the process of reducing the precision of a model’s numerical values to save memory and speed up processing. In computing, numbers are stored with a specific "bit-depth." High precision (32-bit) uses a massive range of decimals for extreme accuracy. Reducing precision means shortening those numbers—moving from long decimals to smaller ones or integers (like 8-bit or 4-bit). </p>
                
                <p><strong>1. Base Model Preparation (4-bit Quantization):</strong> Load the base model using 4-bit NormalFloat (NF4). Unlike standard LoRA, which often uses 16-bit or 8-bit, QLoRA compresses the frozen weights ($W$) into a specialized 4-bit format. This NF4 data type is mathematically optimized for normally distributed weights, ensuring minimal information loss while slashing VRAM requirements by up to 75% compared to full-precision tuning.</p>
                
                <p><strong>2. Configuration (The Ontology):</strong> The LoraConfig remains largely the same, but with two critical QLoRA-specific additions handled via BitsAndBytesConfig: Double Quantization (This quantizes the quantization constants themselves, saving ~0.37 bits per parameter). Paged Optimizers (This acts as a "safety valve." If you hit a memory spike, QLoRA offloads the optimizer states to the CPU RAM to prevent "Out of Memory" crashes).</p>
                
                <p><strong>3. Matrix Injection & The Adapter:</strong> Inject the trainable matrices $A$ and $B$ alongside the frozen model. The base model remains in its 4-bit "frozen" state. The adapters ($A$ and $B$) are created in high-resolution Bfloat. During the forward pass, the model performs a "temporary inflation": it dequantizes 4-bit weights into 16-bit just long enough to perform math with the adapters, then "deflates" them back to 4-bit storage.</p>
                
                <p><strong>4. Training (The Computational Trade-off):</strong> During backpropagation, only the high-precision adapters are updated. The base model acts like a static, low-resolution anchor. Because the system has to constantly inflate (dequantize) and deflate (re-quantize) the base weights for every calculation, it takes about 30% longer than standard LoRA. You are essentially sacrificing processing speed to ensure the model fits on a smaller GPU.</p>
                
                <p><strong>5. Merging & Inference:</strong> You still only save the tiny Adapter weights (<100MB). $W_{final} = Dequantize(W_{4bit}) + (B \times A \times \alpha/r)$. Because the base model is quantized, you typically do not “hard merge” the weights into a single 4-bit file for production if you want to maintain maximum precision. Instead, you load the base 4-bit model and the adapter separately at runtime.</p>
            </div>
        </div>
    </details>

    <details class="section">
        <summary>5. Preference Alignment (DPO)</summary>
        <div class="section-content">
            <p>Preference alignment addresses the shortcomings of SFT by incorporating direct human or AI feedback into the training process. If SFT is the foundation of competence (getting the answer right), preference data is the layer of alignment (getting the answer right for the specific user, context, or value system).</p>
            
            <div class="subsection">
                <span class="subsection-title">Direct Preference Optimization (DPO)</span>
                <p>Maximize the accuracy, diversity, and complexity of our samples. The algorithm calculates the probability of the model generating the "chosen" response versus the "rejected" response. The goal is to maximize the margin between the two. The model learns not just "what to say," but specifically "what not to say" in the context of what it should say. </p>
                
                <p><strong>Data Quantity:</strong> DPO is sample-efficient, meaning it requires significantly less data than SFT to fundamentally change a model’s behavior.</p>
                <ul>
                    <li><strong>General-Purpose (The Giants):</strong> Industry leaders use millions of pairs (often synthetic) to push the limits of reasoning and global safety.</li>
                    <li><strong>Open-Source Standard:</strong> 10k–100k samples are typically used to boost benchmark scores or "heal" a model’s logic after it has been merged or pruned.</li>
                    <li><strong>Task-Specific (The Sniper):</strong> Narrow objectives—like changing a model's persona or enforcing a specific writing style—can be achieved with as few as 100–10,000 pairs.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Generating Preferences</span>
                <ul>
                    <li><strong>LLM-Human:</strong> AI generates options, humans rank them. This is a high-value hybrid; it is easier for humans to judge quality than to create it from scratch, ensuring efficiency and accuracy.</li>
                    <li><strong>LLM-LLM (Synthetic):</strong> Fully automated. It is the most scalable and cheapest option but requires strict oversight to avoid amplifying model biases.</li>
                </ul>
                <p><strong>Model Contrasting:</strong> You prompt two models with one input: a "teacher" (e.g., GPT-4) and a smaller model. The teacher’s output is labeled chosen and the smaller one rejected. This creates a clear signal of high-quality reasoning versus mediocre attempts, teaching the target model to recognize the specific "delta" in quality, logic, and formatting.</p>
                <p><strong>Human-AI Benchmarking in Practice:</strong> In practice, you take a "gold standard" response written by a human and pair it with a draft from your model. You label the human version as preferred and the AI version as rejected.</p>
                <ul>
                     <li><strong>Authenticity:</strong> It forces the model to move past generic "AI-speak" and adopt a specific human tone.</li>
                     <li><strong>Edge Cases:</strong> It identifies where models fail at nuanced tasks, like humor or high-level professional jargon.</li>
                     <li><strong>Style Mimicry:</strong> Essential when you need a model to sound like a specific brand or technical expert.</li>
                </ul>
            </div>
        </div>
    </details>

</div>
</div>
</body>
</html>