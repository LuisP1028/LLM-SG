<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Part IV: Supervised Fine-Tuning // Study Guide</title>
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style>
    :root {
        --bg-color: #000000;
        --text-color: #00ff41;
        --accent-color: #00ff41;
        --dim-color: #003b00;
        --border-color: #00ff41;
        --font-main: 'Courier New', Courier, monospace;
        --font-header: 'Arial Black', Impact, sans-serif;
    }

    * { box-sizing: border-box; }

    body {
        margin: 0;
        padding: 0;
        background-color: var(--bg-color);
        color: var(--text-color);
        font-family: var(--font-main);
        line-height: 1.5;
        overflow-x: hidden;
    }

    /* --- VISUALS --- */
    .dither-layer {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        z-index: -1;
        background-image: radial-gradient(circle, #003b00 1px, transparent 1px);
        background-size: 4px 4px;
        opacity: 0.4;
    }

    .scanlines {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        background: linear-gradient(to bottom, rgba(0, 255, 65, 0), rgba(0, 255, 65, 0) 50%, rgba(0, 20, 0, 0.2) 50%, rgba(0, 20, 0, 0.2));
        background-size: 100% 4px;
        pointer-events: none;
        z-index: 9999;
    }

    .container {
        max-width: 900px;
        width: 100%;
        margin: 0 auto;
        padding: 40px 20px;
        border-left: 2px dashed var(--dim-color);
        border-right: 2px dashed var(--dim-color);
        background-color: rgba(0, 10, 0, 0.9);
        min-height: 100vh;
    }

    /* --- TYPOGRAPHY --- */
    h1 {
        font-family: var(--font-header);
        text-transform: uppercase;
        font-size: 2.5rem;
        border-bottom: 5px solid var(--accent-color);
        margin-bottom: 40px;
        color: var(--accent-color);
        text-align: center;
        text-shadow: 0px 0px 8px var(--accent-color);
        word-wrap: break-word;
    }

    h3 { margin-top: 0; color: var(--accent-color); text-transform: uppercase; }
    strong { color: var(--accent-color); text-decoration: underline; }
    em { font-style: normal; color: #50c878; border-bottom: 1px dotted var(--dim-color); }

    /* --- ACCORDION STYLES --- */
    /* Outer Parts */
    details.part {
        margin-bottom: 30px;
        border: 2px solid var(--border-color);
        background: #000;
        box-shadow: 6px 6px 0px var(--dim-color);
        transition: transform 0.1s;
    }
    details.part[open] { box-shadow: 4px 4px 0px var(--dim-color); transform: translate(2px, 2px); }
    details.part > summary {
        font-family: var(--font-header);
        font-size: 1.5rem;
        padding: 15px 20px;
        background-color: var(--accent-color);
        color: var(--bg-color);
        cursor: pointer;
        list-style: none;
        text-transform: uppercase;
        position: relative;
    }
    details.part > summary::-webkit-details-marker { display: none; }
    details.part > summary::after { content: '+'; position: absolute; right: 20px; font-weight: 900; }
    details.part[open] > summary::after { content: '-'; }
    .part-content { padding: 20px; border-top: 2px solid var(--border-color); }

    /* Inner Sections */
    details.section {
        margin-bottom: 15px;
        border: 1px solid var(--dim-color);
        background: #050505;
    }
    details.section > summary {
        font-family: var(--font-main);
        font-weight: bold;
        padding: 12px;
        background: #0a0a0a;
        color: var(--text-color);
        cursor: pointer;
        list-style: none;
        border-bottom: 1px solid transparent;
        text-transform: uppercase;
        font-size: 1.1rem;
    }
    details.section > summary:hover { background: var(--dim-color); color: var(--accent-color); }
    details.section[open] > summary {
        border-bottom: 1px solid var(--dim-color);
        background: #0f0f0f;
        color: var(--accent-color);
        text-shadow: 0px 0px 5px var(--accent-color);
    }
    .section-content { padding: 20px; }

    /* Subsections */
    .subsection {
        margin-bottom: 25px;
        border-left: 4px solid var(--dim-color);
        padding-left: 15px;
    }
    .subsection-title {
        background: var(--dim-color);
        color: var(--accent-color);
        padding: 2px 6px;
        font-weight: bold;
        text-transform: uppercase;
        display: inline-block;
        margin-bottom: 10px;
        font-size: 0.9rem;
    }

    p { margin-bottom: 12px; margin-top: 0; text-align: justify; }
    ul { padding-left: 20px; margin-bottom: 15px; }
    li { margin-bottom: 5px; }

    .code-block {
        background: #020a02;
        border: 1px dashed var(--dim-color);
        padding: 10px;
        margin: 10px 0;
        font-family: 'Courier New', monospace;
        color: var(--accent-color);
        overflow-x: auto;
        white-space: pre-wrap;
    }

    /* --- INTERACTIVE: EYE BUTTON & VIEWPORT --- */
    
    .eye-btn {
        position: relative;
        width: 28px; height: 28px;
        background: #000;
        border: 1px solid var(--accent-color);
        cursor: pointer;
        padding: 4px;
        display: inline-flex;
        align-items: center; justify-content: center;
        margin-left: 8px;
        vertical-align: bottom;
        transition: transform 0.1s;
    }
    .eye-btn svg { width: 100%; height: 100%; fill: var(--accent-color); }
    .eye-btn:hover {
        background: var(--accent-color);
        transform: translate(-1px, -1px);
        box-shadow: 2px 2px 0px var(--dim-color);
    }
    .eye-btn:hover svg { fill: #000; }

    .dither-bg {
        background-image: 
            linear-gradient(45deg, var(--dim-color) 25%, transparent 25%), 
            linear-gradient(-45deg, var(--dim-color) 25%, transparent 25%), 
            linear-gradient(45deg, transparent 75%, var(--dim-color) 75%), 
            linear-gradient(-45deg, transparent 75%, var(--dim-color) 75%);
        background-size: 4px 4px;
    }

    .retro-viewport {
        position: fixed;
        top: 50%; left: 50%;
        transform: translate(-50%, -50%);
        width: 80vw; height: 80vh;
        max-width: 900px; max-height: 700px;
        background-color: #000;
        border: 2px solid var(--accent-color);
        box-shadow: 0 0 50px rgba(0, 50, 0, 0.8);
        display: flex; flex-direction: column;
        z-index: 10000;
        visibility: hidden; opacity: 0;
        pointer-events: none;
        transition: opacity 0.2s;
        resize: both; overflow: hidden;
    }
    .retro-viewport.active { visibility: visible; opacity: 1; pointer-events: auto; }

    .vp-header {
        background: var(--accent-color);
        color: #000;
        padding: 5px 10px;
        font-weight: bold;
        font-family: var(--font-header);
        display: flex; justify-content: space-between;
        align-items: center;
        border-bottom: 2px solid #000;
        cursor: default;
    }
    .vp-close {
        background: #000; color: var(--accent-color);
        border: 1px solid #000; font-weight: 900; 
        cursor: pointer; font-family: var(--font-main);
    }
    .vp-close:hover { background: #fff; color: #000; }
    
    .vp-body { flex-grow: 1; position: relative; background: #000; }
    .vp-body iframe { width: 100%; height: 100%; border: none; }

    /* --- RESPONSIVE --- */
    @media (max-width: 600px) {
        h1 { font-size: 1.8rem; border-bottom-width: 3px; }
        details.part > summary { font-size: 1.1rem; padding: 12px; }
        details.section > summary { font-size: 0.9rem; }
        .container { padding: 10px; border: none; }
        .part-content, .section-content { padding: 10px; }
        p { text-align: left; }
        .retro-viewport { width: 95vw; height: 60vh; }
    }
</style>
</head>
<body>
<div class="dither-layer"></div>
<div class="scanlines"></div>
<div class="container">
<h1>Part IV: Supervised Fine-Tuning</h1>
<div class="part-content">
    
    <details class="section" open> <summary>1. Preparations for SFT & Data Curation</summary> <div class="section-content"> <p>Refines the model’s capabilities using carefully curated pairs of instructions and corresponding answers.</p>

        <details class="subsection-accordion">
            <summary>Essential Dimensions of Data</summary>
            <div class="accordion-content">
                <ul>
                    <li>
                        <strong>Accuracy:</strong> Accuracy ensures that the data is factually correct and directly answers the user's specific instruction. It is crucial for building trust, as the model must learn to provide reliable, true information rather than incorrect facts or irrelevant responses.
                    </li>
                    <li>
                        <strong>Diversity:</strong> Diversity means the dataset must cover a wide variety of topics, writing styles, and lengths. It should represent the many different ways people will actually use the model. This variety helps the AI become flexible enough to handle new situations and valid requests.
                    </li>
                    <li>
                        <strong>Complexity:</strong> Complexity avoids easy or trivial examples that do not challenge the model. Instead, the data should include difficult tasks and multi-step reasoning problems. Training on harder material pushes the AI to develop the logic needed to solve real-world problems.
                    </li>
                    <li>
                        <strong>Data Quantity:</strong> The Hugging Face Hub contains numerous instruction datasets, which can be general-purpose or designed for particular tasks or domains.
                    </li>
                </ul>
            </div>
        </details>
    
        <details class="subsection-accordion">
            <summary>Task vs. Domain Specific Models</summary>
            <div class="accordion-content">
                <p><strong>Task-specific</strong> models focus on performing a single action, such as translation or summarization, often requiring less data and smaller model sizes. For task-specific models, data curation often involves collecting examples of the desired task from existing datasets or creating new ones.</p>
                <p><strong>Domain-specific</strong> models, however, focus on mastering a specific industry, like law or medicine. They prioritize specialized knowledge and vocabulary, with data needs varying based on the field's complexity. Domains that are well-represented in the original training data may require less fine-tuning, while those that are more specialized or underrepresented may need more extensive datasets.</p>
            </div>
        </details>
    
        <details class="subsection-accordion">
            <summary>Data Curation: Rule-Based Filtering</summary>
            <div class="accordion-content">
                <p><strong>Rule-based filtering</strong> is a quality control method that uses explicit, pre-set rules to automatically keep or remove data samples. It aims to maintain high standards by strictly enforcing specific criteria. Its speed and efficiency allow for rapid application to large volumes of data, making it highly scalable.</p>
                <ul>
                    <li><strong>Length filtering:</strong> This rule manages data quality by setting minimum and maximum size limits. It removes responses that are too short to be meaningful or too long and repetitive, though the specific limits depend on the task (e.g., a summary vs. a detailed explanation).</li>
                    <li><strong>Keyword exclusion:</strong> This rule filters data based on specific content. It scans for and removes samples containing banned words—such as profanity, spam, or slang—to ensure the data matches the intended tone and creates a safe environment.</li>
                    <li><strong>Format checking:</strong> This rule validates the structure of the data. It ensures that samples, particularly technical ones like computer code or JSON, follow specific formatting requirements so they remain consistent and usable.</li>
                </ul>
            </div>
        </details>
    
        <details class="subsection-accordion">
            <summary>Data Deduplication & Decontamination</summary>
            <div class="accordion-content">
                <p>In practice, these steps are the "safeguards" of a pipeline, ensuring the model doesn't just memorize patterns or "leak" answers. Engineers use <strong>MinHash LSH</strong> to scale this across millions of rows efficiently. <strong>Fuzzy Deduplication</strong> identifies "near-misses"—like the same recipe with different line breaks—and deletes them to prevent the model from overfitting on repetitive content. <strong>Decontamination</strong> acts as a "cheat check," where the training set is compared against benchmark test sets (like MMLU); any training sample too similar to a test question is purged to ensure the final evaluation measures true reasoning, not memorization.</p>
    
                
    
                <h3>Exact Deduplication</h3>
                <p><strong>Exact deduplication</strong> removes identical entries by normalizing data and comparing unique hash codes (e.g., <strong>SHA-256</strong>). If hashes match, the data is considered a duplicate.</p>
                <ul>
                    <li><strong>Precision:</strong> Extremely high for identical copies.</li>
                    <li><strong>Limitation:</strong> It is "brittle"—it misses documents with even a single character difference, typos, or semantically similar content.</li>
                </ul>
    
                <hr>
    
                <h3>Fuzzy Deduplication</h3>
                <p><strong>Fuzzy deduplication</strong> detects <em>near-duplicates</em> using <strong>MinHash</strong>. This process involves breaking data into "shingle" fingerprints to compare documents that are similar but not identical.</p>
    
                <h4>1. Shingling & N-grams</h4>
                <p><strong>Shingles</strong> are overlapping subsequences used to convert text into a mathematical set. For example, the phrase "red car" might become the set: <code>{"red", "ed ", " ca", "car"}</code>.</p>
                <p>These are often categorized as <strong>n-grams</strong> (contiguous sequences of <em>n</em> items):</p>
                <ul>
                    <li><strong>Character n-gram:</strong> "Code" (n=3) -> <code>{"Cod", "ode"}</code></li>
                    <li><strong>Word n-gram:</strong> "My code runs" (n=2) ->  <code>{"My code", "code runs"}</code></li>
                </ul>
    
                <h4>2. Jaccard Similarity Index</h4>
                <p>The <strong>Jaccard Similarity Index</strong> measures the resemblance between two sets. It is calculated by dividing the size of the <strong>intersection</strong> (shared shingles) by the size of the <strong>union</strong> (total unique shingles).</p>
    
                
    
                <blockquote>
                    The formula for Jaccard Similarity is:
                    $$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$$
                </blockquote>
    
                <ul>
                    <li>A score of <strong>1.0</strong> indicates identical sets (complete overlap).</li>
                    <li>A score of <strong>0.0</strong> indicates no overlap whatsoever.</li>
                </ul>
            </div>
        </details>
    
        <details class="subsection-accordion">
            <summary>Data Quality Evaluation (LLM-as-a-Judge)</summary>
            <div class="accordion-content">
                <p>Measures attributes like accuracy, diversity, and complexity to ensure datasets are fit for training models. To avoid the high cost of human annotation, developers increasingly use <strong>LLM-as-a-judge</strong> workflows to automate this assessment.</p>
        
                <h3>LLM-as-a-Judge: Scoring vs. Ranking</h3>
                <p>When using an LLM to evaluate preference data, there are two primary methods:</p>
                <ul>
                    <li><strong>Absolute Scoring:</strong> The LLM assigns a 1–5 or "Good/Bad" rating to a single response. It’s fast but often inconsistent across different prompts or sessions.</li>
                    <li><strong>Pairwise Ranking [Best]:</strong> The LLM compares two responses (A vs. B) to pick a winner. This is the gold standard for DPO because it mimics human decision-making and produces more stable, relative signals.</li>
                </ul>
        
                <h3>Boosting Evaluation Accuracy</h3>
                <ul>
                    <li><strong>CoT + Pairwise Ranking:</strong> Use when the task is highly subjective or creative (e.g., poetry, persona-mimicry).
                        <br><em>Strength:</em> Forces logical justification. Since no "answer key" exists, the judge's own reasoning process ensures it weighs complex trade-offs—like creativity vs. clarity—before picking a winner. It prevents reliance on "vibes" or length by building an independent logical path.
                    </li>
                    <li><strong>Ground Truth + Pairwise Ranking:</strong> Use when there is a definitive "correct" answer (e.g., Python scripts, calculus, or history).
                        <br><em>Strength:</em> Acts as an immutable anchor. It eliminates hallucinations by providing the judge with the "answer key," forcing it to penalize confident but incorrect logic. It shifts the task from "Which sounds better?" to "Which is more correct?"
                    </li>
                    <li><strong>The Rubric + Pairwise Ranking:</strong> Use when the task is constrained by specific requirements but lacks a single "right" answer (e.g., "Summarize this for a 5-year-old").
                        <br><em>Strength:</em> Prevents "goalpost shifting." By defining the ideal criteria first, the judge resists favoring superficial traits like politeness or length over actual compliance, leading to a much cleaner training signal.
                    </li>
                </ul>
        
                <p><strong>Biases of LLM-as-a-judge:</strong></p>
                <ul>
                    <li><strong>Position Bias:</strong> Favoring the first answer presented.</li>
                    <li><strong>Verbosity Bias:</strong> Preferring longer responses.</li>
                    <li><strong>Self-Preference:</strong> Rating their own model family higher.</li>
                </ul>
                <p>Counter these by randomizing order, length normalization, and using diverse judges.</p>
        
                <h3>Reinforcement Learning from Human Feedback (RLHF)</h3>
                <p>RLHF is most effective when the "ground truth" is difficult to define mathematically but easy for a human to recognize. The Bradley-Terry model turns a simple "A is better than B" vote into a mathematical probability. If a human prefers response $A$ over $B$, the loss function penalizes the model if its predicted score for $A$ isn't higher than its score for $B$.</p>
        
                <ol>
                    <li><strong>The Reward Model (Learning "Taste"):</strong> The Reward Model (RM) is typically a sibling of the candidate model (same architecture) but modified with a scalar regression head. Instead of predicting the next word, it outputs a single number. Using a Bradley-Terry loss, the RM learns to distinguish the features—logic, tone, safety—that humans prefer.</li>
                    <li><strong>Policy Selection (PPO):</strong> Updates $\pi_\theta$ (parameters) via on-policy optimization. Its $KL-penalty$ mechanism enforces a constraint, providing a stable framework for aligning weights with the optimal policy.</li>
                    <li><strong>How the RM Informs the Candidate:</strong> The candidate model generates a response. The RM "grades" it with a score. PPO then calculates the <strong>Advantage</strong>: how much better this response was compared to the model's average.
                        <ul>
                            <li><strong>Positive score:</strong> The model is nudged to produce similar outputs.</li>
                            <li><strong>Negative score:</strong> The model is nudged away.</li>
                        </ul>
                    </li>
                </ol>
            </div>
        </details>
    
        <details class="subsection-accordion">
            <summary>Data Exploration & Generation</summary>
            <div class="accordion-content">
                <p><strong>Data Exploration:</strong> Statistical analysis uses NLP libraries like NLTK and spaCy. Topic clustering converts text into numerical embeddings using sentence transformers (e.g., DBSCAN) to group vectors based on proximity.</p>
                <p><strong>Data Generation:</strong> Synthetic data pipelines generate and validate training sets starting with a <strong>taxonomy</strong> (the organized map of skills) and <strong>seeds</strong> (the prompt templates used to generate the data).</p>
            </div>
        </details>
    
        <details class="subsection-accordion">
            <summary>Data Augmentation (Evol-Instruct & UltraFeedback)</summary>
            <div class="accordion-content">
                <p>In this context, <strong>data augmentation</strong> refers to the process of increasing both the quantity and the quality of data samples. While generation creates data from scratch, augmentation takes actual, existing samples and transforms them (e.g., rewording) to add variety or fix errors.</p>
                <hr>
                
                <h4>1. Evol-Instruct (Upgrading the Input)</h4>
                <p><strong>Evol-Instruct</strong> focuses on the <strong>Question</strong>. It rewrites simple prompts into complex ones to force the model to think harder or broaden its scope.</p>
                <ul>
                    <li><strong>In-Depth Evolving (Making it Harder):</strong> Increases the difficulty of a specific prompt.
                        <ul>
                            <li><em>Constraints:</em> Adding strict rules or limits.</li>
                            <li><em>Deepening:</em> Asking for comprehensive analysis rather than simple facts.</li>
                            <li><em>Concretizing:</em> Making vague concepts specific.</li>
                            <li><em>Reasoning Steps:</em> Demanding multi-step logic.</li>
                            <li><em>Complicating Input:</em> Adding complex formats like code or JSON.</li>
                        </ul>
                    </li>
                    <li><strong>In-Breadth Evolving (Making it Diverse):</strong> Increases variety. It generates completely new, unique instructions inspired by the original ones, focusing on rare or niche topics to prevent the dataset from being repetitive.</li>
                </ul>
        
                <h4>2. UltraFeedback (Upgrading the Output)</h4>
                <p><strong>UltraFeedback</strong> upgrades the <strong>Answer</strong>. It keeps the prompt the same but generates multiple responses from different models. An <strong>LLM as a Judge</strong> then rates these responses to select the highest-quality answer for training.</p>
                <ul>
                    <li><strong>The Inputs:</strong> You generate multiple answers (e.g., 4) for one question using different models.</li>
                    <li><strong>The Rubric:</strong> You give a "Judge" LLM a specific scorecard (e.g., "Score 1-5 on adherence to military safety protocols").</li>
                    <li><strong>The Selection:</strong> The Judge reads the text, scores it based on that rubric, and keeps only the highest-scoring answer.</li>
                </ul>
        
                <p><em><strong>Summary:</strong> One builds better prompts (Evol-Instruct); the other curates better answers (UltraFeedback).</em></p>
            </div>
        </details>
    </div>
    </details>

    <details class="section">
        <summary>2. Evol Instruct Examples</summary>
        <div class="section-content">
            <div class="subsection">
                <span class="subsection-title">Infrastructure</span>
                <ul>
                    <li><strong>Base Instruction:</strong> Inspect the bridge for damage.</li>
                    <li><strong>In-Depth (Constraint):</strong> Inspect the bridge for micro-fractures in the support pylons using thermal imaging data, citing specific ISO safety standards for load-bearing capacity.</li>
                    <li><strong>In-Breadth (New Topic):</strong> Develop a predictive maintenance schedule for an aging underground sewer network based on soil acidity levels.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Defense</span>
                <ul>
                    <li><strong>Base Instruction:</strong> Analyze satellite imagery.</li>
                    <li><strong>In-Depth (Complicating Input):</strong> Analyze the provided raw JSON telemetry from a surveillance drone to identify camouflaged vehicle heat signatures, ignoring background radiation noise.</li>
                    <li><strong>In-Breadth (New Topic):</strong> Formulate a jamming protocol to disrupt enemy communication channels during a high-altitude paratrooper insertion.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Healthcare</span>
                <ul>
                    <li><strong>Base Instruction:</strong> Diagnose the patient's fever.</li>
                    <li><strong>In-Depth (Reasoning):</strong> Diagnose the fever in a post-transplant patient exhibiting rejection signs, differentiating it from a bacterial infection using the provided blood panel results.</li>
                    <li><strong>In-Breadth (New Topic):</strong> Outline a triage procedure for a mass-casualty event involving exposure to a rare neurotoxin in a metropolitan area.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Aerospace</span>
                <ul>
                    <li><strong>Base Instruction:</strong> Check the landing gear.</li>
                    <li><strong>In-Depth (Constraint):</strong> Troubleshoot the landing gear hydraulic failure occurring specifically at -40°C, proposing a fix that adds no more than 5kg of weight to the assembly.</li>
                    <li><strong>In-Breadth (New Topic):</strong> Calculate the trajectory correction burn required for a probe entering a polar orbit around Europa.</li>
                </ul>
            </div>
        </div>
    </details>

    <details class="section">
        <summary>3. Creating an Instruction Dataset</summary>
        <div class="section-content">
            
            <div class="subsection">
                <span class="subsection-title">Instruction Dataset Formats</span>
                <p>Instruction datasets organize text for AI training.</p>
                <ul>
                    <li><strong>Alpaca:</strong> format is designed for single-turn tasks (one prompt, one reply).</li>
                    <li><strong>ShareGPT or OpenAI:</strong> for multi-turn conversations are superior because they track dialogue history using lists of messages.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Chat Templates</span>
                <p>Chat templates are formatting schemas that use special tokens to structure dialogue so the model understands who is speaking (e.g., User vs. Assistant). Their usage depends on the model type:</p>
                <ul>
                    <li><strong>Base Model:</strong> A raw model trained only to predict the next word in a sequence. It has no inherent conversation structure, so you can choose any template (like ChatML) when preparing it for training.</li>
                    <li><strong>Instruct Model:</strong> A base model that has been "fine-tuned" to follow instructions. This training process bakes in a specific template. You must reuse that exact template, or the model will lose the context of the conversation.</li>
                </ul>
                <p>Chat templates rigorously structure text into three roles: system, user, and assistant.</p>
            </div>
        </div>
    </details>

    <details class="section">
        <summary>4. Fine Tuning Methods (LoRA & QLoRA)</summary>
        <div class="section-content">
            
            <div class="subsection">
                <span class="subsection-title">LoRA - Low Rank Adaptation</span>
                <p>Instead of retraining a model’s entire massive web of parameters—which requires immense computational power—LoRA keeps the original pre-trained weights ($W$) frozen. </p>
                
                <p><strong>1. Base Model Preparation:</strong> Load your pre-trained model (the "Base Model") in a quantized format (e.g., 4-bit) to save memory. At this stage, the model is standard. You must "freeze" the model, ensuring no original parameters will be updated during training. The PEFT library handles this freezing automatically when you apply the configuration.</p>
                
                <p><strong>2. Configuration (The Ontology):</strong> You must define a LoraConfig object to dictate how the adaptation works.</p>
                <ul>
                    <li><strong>r (Rank):</strong> Set the dimension of the low-rank matrices ($A$ and $B$). Start with 8 or 16. Higher ranks (e.g., 64) allow for more complex learning but increase VRAM usage.</li>
                    <li><strong>lora_alpha (Alpha):</strong> Set the scaling factor. A standard heuristic is $\alpha = 2 \times r$ (e.g., if rank is 16, set alpha to 32). This ensures the update is strong enough to influence the model.</li>
                    <li><strong>target_modules:</strong> List the specific layers to attach LoRA to. For Style/Format: Target ["q_proj", "v_proj"] (Attention Query/Value). For Reasoning/Facts: Target ["gate_proj", "up_proj", "down_proj"] (MLP layers) in addition to attention.</li>
                    <li>Matrix $A$ compresses the input down to the small rank (r). Matrix $B$ expands it back up to the original size.</li>
                </ul>
                
                <p><strong>3. Model Injection:</strong> Apply the configuration to create the parallel adapter structure. Result: This attaches small, trainable matrices ($A$ and $B$) to the frozen layers. Initially representing zero change, $A$ and $B$ are updated during backpropagation to learn the delta ($\Delta W$)—the precise mathematical offset required to steer the model's output. Because $A$ and $B$ are structurally small (low-rank), they capture this task-specific adjustment using only a fraction of the total parameters.</p>
                
                <p><strong>4. Training (Fine-Tuning):</strong> Run your standard training loop (e.g., using Hugging Face Trainer). During backpropagation, the optimizer ignores the frozen base weights and only updates the specific LoRA matrices you injected. This prevents "catastrophic forgetting" of the base model's knowledge.</p>
                
                <p><strong>5. Merging & Inference:</strong> Save only the Adapter (the LoRA weights), which is small (usually <100MB). To use the model, load the Base Model again and strictly "merge" the saved Adapter weights into it. The math is: $W_{final} = W_{base} + (B \times A \times \alpha/r)$. We add the results together because LoRA is an additive modification rather than a replacement. In the Transformer architecture, the original weights ($W$) represent the model's vast, general knowledge. When you fine-tune for a specific task, you don't want to discard that foundation; you want to nudge it.</p>
            </div>

            <div class="subsection">
                <span class="subsection-title">QLoRA</span>
                <p>Think of QLoRA as LoRA with a world-class packing strategy. It maintains the same low-rank logic but adds layers of compression to make massive models fit on modest hardware.</p>
                <p><strong>Quantization</strong> is the process of reducing the precision of a model’s numerical values to save memory and speed up processing. In computing, numbers are stored with a specific "bit-depth." High precision (32-bit) uses a massive range of decimals for extreme accuracy. Reducing precision means shortening those numbers—moving from long decimals to smaller ones or integers (like 8-bit or 4-bit). </p>
                
                <p><strong>1. Base Model Preparation (4-bit Quantization):</strong> Load the base model using 4-bit NormalFloat (NF4). Unlike standard LoRA, which often uses 16-bit or 8-bit, QLoRA compresses the frozen weights ($W$) into a specialized 4-bit format. This NF4 data type is mathematically optimized for normally distributed weights, ensuring minimal information loss while slashing VRAM requirements by up to 75% compared to full-precision tuning.</p>
                
                <p><strong>2. Configuration (The Ontology):</strong> The LoraConfig remains largely the same, but with two critical QLoRA-specific additions handled via BitsAndBytesConfig: Double Quantization (This quantizes the quantization constants themselves, saving ~0.37 bits per parameter). Paged Optimizers (This acts as a "safety valve." If you hit a memory spike, QLoRA offloads the optimizer states to the CPU RAM to prevent "Out of Memory" crashes).</p>
                
                <p><strong>3. Matrix Injection & The Adapter:</strong> Inject the trainable matrices $A$ and $B$ alongside the frozen model. The base model remains in its 4-bit "frozen" state. The adapters ($A$ and $B$) are created in high-resolution Bfloat. During the forward pass, the model performs a "temporary inflation": it dequantizes 4-bit weights into 16-bit just long enough to perform math with the adapters, then "deflates" them back to 4-bit storage.</p>
                
                <p><strong>4. Training (The Computational Trade-off):</strong> During backpropagation, only the high-precision adapters are updated. The base model acts like a static, low-resolution anchor. Because the system has to constantly inflate (dequantize) and deflate (re-quantize) the base weights for every calculation, it takes about 30% longer than standard LoRA. You are essentially sacrificing processing speed to ensure the model fits on a smaller GPU.</p>
                
                <p><strong>5. Merging & Inference:</strong> You still only save the tiny Adapter weights (<100MB). $W_{final} = Dequantize(W_{4bit}) + (B \times A \times \alpha/r)$. Because the base model is quantized, you typically do not “hard merge” the weights into a single 4-bit file for production if you want to maintain maximum precision. Instead, you load the base 4-bit model and the adapter separately at runtime.</p>
            </div>
        </div>
    </details>

    <details class="section">
        <summary>5. Preference Alignment (DPO)</summary>
        <div class="section-content">
            <p>Preference alignment addresses the shortcomings of SFT by incorporating direct human or AI feedback into the training process. If SFT is the foundation of competence (getting the answer right), preference data is the layer of alignment (getting the answer right for the specific user, context, or value system).</p>
            
            <div class="subsection">
                <span class="subsection-title">Direct Preference Optimization (DPO)</span>
                <p>Maximize the accuracy, diversity, and complexity of our samples. The algorithm calculates the probability of the model generating the "chosen" response versus the "rejected" response. The goal is to maximize the margin between the two. The model learns not just "what to say," but specifically "what not to say" in the context of what it should say. </p>
                
                <p><strong>Data Quantity:</strong> DPO is sample-efficient, meaning it requires significantly less data than SFT to fundamentally change a model’s behavior.</p>
                <ul>
                    <li><strong>General-Purpose (The Giants):</strong> Industry leaders use millions of pairs (often synthetic) to push the limits of reasoning and global safety.</li>
                    <li><strong>Open-Source Standard:</strong> 10k–100k samples are typically used to boost benchmark scores or "heal" a model’s logic after it has been merged or pruned.</li>
                    <li><strong>Task-Specific (The Sniper):</strong> Narrow objectives—like changing a model's persona or enforcing a specific writing style—can be achieved with as few as 100–10,000 pairs.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Generating Preferences</span>
                <ul>
                    <li><strong>LLM-Human:</strong> AI generates options, humans rank them. This is a high-value hybrid; it is easier for humans to judge quality than to create it from scratch, ensuring efficiency and accuracy.</li>
                    <li><strong>LLM-LLM (Synthetic):</strong> Fully automated. It is the most scalable and cheapest option but requires strict oversight to avoid amplifying model biases.</li>
                </ul>
                <p><strong>Model Contrasting:</strong> You prompt two models with one input: a "teacher" (e.g., GPT-4) and a smaller model. The teacher’s output is labeled chosen and the smaller one rejected. This creates a clear signal of high-quality reasoning versus mediocre attempts, teaching the target model to recognize the specific "delta" in quality, logic, and formatting.</p>
                <p><strong>Human-AI Benchmarking in Practice:</strong> In practice, you take a "gold standard" response written by a human and pair it with a draft from your model. You label the human version as preferred and the AI version as rejected.</p>
                <ul>
                     <li><strong>Authenticity:</strong> It forces the model to move past generic "AI-speak" and adopt a specific human tone.</li>
                     <li><strong>Edge Cases:</strong> It identifies where models fail at nuanced tasks, like humor or high-level professional jargon.</li>
                     <li><strong>Style Mimicry:</strong> Essential when you need a model to sound like a specific brand or technical expert.</li>
                </ul>
            </div>
        </div>
    </details>

</div>
</div>
</body>
</html>