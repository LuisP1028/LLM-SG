<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Part I: BASICS // Study Guide</title>
<style>
    :root {
        --bg-color: #000000;
        --text-color: #00ff41;
        --accent-color: #00ff41;
        --dim-color: #003b00;
        --border-color: #00ff41;
        --font-main: 'Courier New', Courier, monospace;
        --font-header: 'Arial Black', Impact, sans-serif;
    }

    * { box-sizing: border-box; }

    body {
        margin: 0;
        padding: 0;
        background-color: var(--bg-color);
        color: var(--text-color);
        font-family: var(--font-main);
        line-height: 1.5;
        overflow-x: hidden;
    }

    /* --- VISUALS --- */
    .dither-layer {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        z-index: -1;
        background-image: radial-gradient(circle, #003b00 1px, transparent 1px);
        background-size: 4px 4px;
        opacity: 0.4;
    }

    .scanlines {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        background: linear-gradient(to bottom, rgba(0, 255, 65, 0), rgba(0, 255, 65, 0) 50%, rgba(0, 20, 0, 0.2) 50%, rgba(0, 20, 0, 0.2));
        background-size: 100% 4px;
        pointer-events: none;
        z-index: 9999;
    }

    .container {
        max-width: 900px;
        width: 100%;
        margin: 0 auto;
        padding: 40px 20px;
        border-left: 2px dashed var(--dim-color);
        border-right: 2px dashed var(--dim-color);
        background-color: rgba(0, 10, 0, 0.9);
        min-height: 100vh;
    }

    /* --- TYPOGRAPHY --- */
    h1 {
        font-family: var(--font-header);
        text-transform: uppercase;
        font-size: 2.5rem;
        border-bottom: 5px solid var(--accent-color);
        margin-bottom: 40px;
        color: var(--accent-color);
        text-align: center;
        text-shadow: 0px 0px 8px var(--accent-color);
        word-wrap: break-word;
    }

    h3 { margin-top: 0; color: var(--accent-color); text-transform: uppercase; }
    strong { color: var(--accent-color); text-decoration: underline; }
    em { font-style: normal; color: #50c878; border-bottom: 1px dotted var(--dim-color); }

    /* --- ACCORDION STYLES --- */
    /* Outer Parts */
    details.part {
        margin-bottom: 30px;
        border: 2px solid var(--border-color);
        background: #000;
        box-shadow: 6px 6px 0px var(--dim-color);
        transition: transform 0.1s;
    }
    details.part[open] { box-shadow: 4px 4px 0px var(--dim-color); transform: translate(2px, 2px); }
    details.part > summary {
        font-family: var(--font-header);
        font-size: 1.5rem;
        padding: 15px 20px;
        background-color: var(--accent-color);
        color: var(--bg-color);
        cursor: pointer;
        list-style: none;
        text-transform: uppercase;
        position: relative;
    }
    details.part > summary::-webkit-details-marker { display: none; }
    details.part > summary::after { content: '+'; position: absolute; right: 20px; font-weight: 900; }
    details.part[open] > summary::after { content: '-'; }
    .part-content { padding: 20px; border-top: 2px solid var(--border-color); }

    /* Inner Sections */
    details.section {
        margin-bottom: 15px;
        border: 1px solid var(--dim-color);
        background: #050505;
    }
    details.section > summary {
        font-family: var(--font-main);
        font-weight: bold;
        padding: 12px;
        background: #0a0a0a;
        color: var(--text-color);
        cursor: pointer;
        list-style: none;
        border-bottom: 1px solid transparent;
        text-transform: uppercase;
        font-size: 1.1rem;
    }
    details.section > summary:hover { background: var(--dim-color); color: var(--accent-color); }
    details.section[open] > summary {
        border-bottom: 1px solid var(--dim-color);
        background: #0f0f0f;
        color: var(--accent-color);
        text-shadow: 0px 0px 5px var(--accent-color);
    }
    .section-content { padding: 20px; }

    /* Subsections */
    .subsection {
        margin-bottom: 25px;
        border-left: 4px solid var(--dim-color);
        padding-left: 15px;
    }
    .subsection-title {
        background: var(--dim-color);
        color: var(--accent-color);
        padding: 2px 6px;
        font-weight: bold;
        text-transform: uppercase;
        display: inline-block;
        margin-bottom: 10px;
        font-size: 0.9rem;
    }

    p { margin-bottom: 12px; margin-top: 0; text-align: justify; }
    ul { padding-left: 20px; margin-bottom: 15px; }
    li { margin-bottom: 5px; }

    .code-block {
        background: #020a02;
        border: 1px dashed var(--dim-color);
        padding: 10px;
        margin: 10px 0;
        font-family: 'Courier New', monospace;
        color: var(--accent-color);
        overflow-x: auto;
        white-space: pre-wrap;
    }

    /* --- INTERACTIVE: EYE BUTTON & VIEWPORT --- */
    
    .eye-btn {
        position: relative;
        width: 28px; height: 28px;
        background: #000;
        border: 1px solid var(--accent-color);
        cursor: pointer;
        padding: 4px;
        display: inline-flex;
        align-items: center; justify-content: center;
        margin-left: 8px;
        vertical-align: bottom;
        transition: transform 0.1s;
    }
    .eye-btn svg { width: 100%; height: 100%; fill: var(--accent-color); }
    .eye-btn:hover {
        background: var(--accent-color);
        transform: translate(-1px, -1px);
        box-shadow: 2px 2px 0px var(--dim-color);
    }
    .eye-btn:hover svg { fill: #000; }

    .dither-bg {
        background-image: 
            linear-gradient(45deg, var(--dim-color) 25%, transparent 25%), 
            linear-gradient(-45deg, var(--dim-color) 25%, transparent 25%), 
            linear-gradient(45deg, transparent 75%, var(--dim-color) 75%), 
            linear-gradient(-45deg, transparent 75%, var(--dim-color) 75%);
        background-size: 4px 4px;
    }

    .retro-viewport {
        position: fixed;
        top: 50%; left: 50%;
        transform: translate(-50%, -50%);
        width: 80vw; height: 80vh;
        max-width: 900px; max-height: 700px;
        background-color: #000;
        border: 2px solid var(--accent-color);
        box-shadow: 0 0 50px rgba(0, 50, 0, 0.8);
        display: flex; flex-direction: column;
        z-index: 10000;
        visibility: hidden; opacity: 0;
        pointer-events: none;
        transition: opacity 0.2s;
        resize: both; overflow: hidden;
    }
    .retro-viewport.active { visibility: visible; opacity: 1; pointer-events: auto; }

    .vp-header {
        background: var(--accent-color);
        color: #000;
        padding: 5px 10px;
        font-weight: bold;
        font-family: var(--font-header);
        display: flex; justify-content: space-between;
        align-items: center;
        border-bottom: 2px solid #000;
        cursor: default;
    }
    .vp-close {
        background: #000; color: var(--accent-color);
        border: 1px solid #000; font-weight: 900; 
        cursor: pointer; font-family: var(--font-main);
    }
    .vp-close:hover { background: #fff; color: #000; }
    
    .vp-body { flex-grow: 1; position: relative; background: #000; }
    .vp-body iframe { width: 100%; height: 100%; border: none; }

    /* --- RESPONSIVE --- */
    @media (max-width: 600px) {
        h1 { font-size: 1.8rem; border-bottom-width: 3px; }
        details.part > summary { font-size: 1.1rem; padding: 12px; }
        details.section > summary { font-size: 0.9rem; }
        .container { padding: 10px; border: none; }
        .part-content, .section-content { padding: 10px; }
        p { text-align: left; }
        .retro-viewport { width: 95vw; height: 60vh; }
    }
</style>
</head>
<body>

<div class="dither-layer"></div>
<div class="scanlines"></div>

<div class="container"> 
    <h1>Part I: Basics</h1>

    <div class="part-content">
        <details class="section">
            <summary>1. The Retrieval Pipeline & Distance</summary>
            <div class="section-content">
                <div class="subsection">
                    <span class="subsection-title">Core Mechanism</span>
                    <p>The retrieval pipeline finds relevant information to answer your question. In essence: your input gets converted to a vector, that vector is matched against a database of vectors, the best matches are retrieved, and those matches become the knowledge base for generating your answer.</p>
                    
                    <ol>
                        <li>When you ask something, the system converts your question into a vector—a list of numbers that captures the meaning of what you said. Your vector database already contains vectors of all your stored information. Each piece of information has been converted into its own vector.</li>
                        <li>The retrieval step compares your question's vector against all the vectors in the database. It finds the K most similar ones—meaning it identifies which stored information has vectors closest to your question's vector.</li>
                        <li>These matching pieces of information are then extracted and added to the prompt sent to the language model. This gives the LLM the specific context it needs to generate an accurate, relevant answer to your question.</li>
                    </ol>
                </div>
    
                <div class="subsection">
                    <span class="subsection-title">Distance Metrics</span>
                    <p>DISTANCE METRICS: Manhattan catches dimension-specific deviations without amplification. Euclidean catches overall spatial distance with emphasis on larger gaps. Cosine catches conceptual similarity regardless of size.</p>
                    <ul>
                        <li>
                            <strong>[Dimensional Differences] Manhattan:</strong> Treats each dimension independently and equally. Good when each dimension represents an independent criterion that matters separately.
                            <br><br>
                            <strong>Example: Satellite Subsystem Anomaly Detection:</strong> Monitoring a constellation’s telemetry by comparing current performance against a "healthy" baseline across independent sensors (e.g., Temperature, Torque, Battery Voltage). Manhattan tracks the cumulative "drift" across all subsystems, ensuring that a critical failure in one component is detected immediately.
                        </li>
                        <li>
                            <strong>[Magnitude] Euclidean:</strong> Sensitive to overall magnitude differences between vectors. Good when the total distance in space matters and dimensions interact.
                            <br><br>
                            <strong>Example: Voice Biometrics:</strong> Matching a speaker’s "voiceprint" in a security system. Because pitch, volume, and resonance interact to form a unique physical signature, the absolute distance between points matters. A slight shift across all dimensions (overall magnitude) indicates a different person, making distance sensitivity crucial.
                        </li>
                        <li>
                            <strong>[Similarity] Cosine:</strong> Ignores magnitude entirely, only measures directional alignment. Two vectors pointing the same way are similar regardless of length. Good when semantic meaning (direction) matters but scale doesn't.
                            <br><br>
                            <strong>Example: Article Length:</strong> A user asks "How do I bake bread?" Cosine matches this query to both a 10-word tip and a 5,000-word encyclopedia entry. Since it only measures the "Bread Baking" direction and ignores the "length" (magnitude), the long article isn't penalized for having more total word-vectors than the query.
                        </li>
                    </ul>
                </div>
            </div>
        </details>

        <details class="section">
            <summary>2. The Feature Pipeline & Data Storage</summary>
            <div class="section-content">
                <div class="subsection">
                    <span class="subsection-title">The Feature Pipeline</span>
                    <p>It contains three main processing steps necessary for fine-tuning and RAG: cleaning, chunking, and embedding. </p>
                    <p>It processes data types differently (e.g. articles, posts, and code).</p>
                    <p>It creates two snapshots of the data, one after cleaning (used for fine-tuning) and one after embedding (used for RAG).</p>
                    <p>The feature pipeline uses a logical feature store instead of a specialized feature store.</p>
                </div>
                <div class="subsection">
                    <span class="subsection-title">Data Storage [Logical/Specialized]</span>
                    <p><strong>Specialized Feature Store:</strong> Vital when injecting <strong>real-time structured data</strong> into prompts.</p>
                    <p><strong>Logical Feature Store:</strong> Useful for storing "generalized knowledge" about "x".</p>
                    <p><strong>The relationship:</strong> Logical provides general knowledge. Specialized stores inject live context to filter or personalize that knowledge.</p>
                    <p><strong>Artifacts:</strong> Versioned, trackable packages of data or models created during development. They act as "saved snapshots" that bundle a file with its history.</p>
                </div>
            </div>
        </details>

        <details class="section"> 
            <summary>3. CDC (Change Data Capture)</summary> 
            <div class="section-content">
                <div class="subsection">
                    <span class="subsection-title">The Role of CDC</span>
                    <p>CDC links the data warehouse (source) and feature store (target) by monitoring the warehouse's transaction logs for specific events.</p>
                    
                    <p>CDC captures the raw data change to trigger the transformation logic immediately. This updates the computed features in the feature store in real-time, avoiding slow bulk re-processing. In other words, when an operation is performed on the data warehouse affecting the transformations performed in the feature store; the affected features in the feature store are immediately recomputed.</p>
                </div>

                <div class="subsection">
                    <span class="subsection-title">Push vs. Pull Strategies</span>
                    <div class="code-block">
PUSH: Sync
In the Push strategy, the source database actively detects changes and immediately transmits them to the target system.
This ensures near real-time synchronization.
To prevent data loss if the target is offline, a messaging system acts as a buffer to hold data until the target is ready.

PULL: Periodic Polling
In the Pull strategy, the source database is passive, simply logging changes.
The target system periodically polls (requests) these logs to update itself.
While this reduces load on the source, it introduces latency.
A messaging system buffers the logs to ensure data safety if the target goes down.
                    </div> 
                </div>
            </div>
        </details>

        <details class="section">
            <summary>4. CDC Implementation Types</summary>
            <div class="section-content">
                <div class="subsection">
                    <span class="subsection-title">Trigger-Based (Precision)</span>
                    <p><strong>Mechanism:</strong> Pre-define SQL triggers that automatically execute when data changes, writing the details to a separate tracking table. It typically records the row's primary key, the operation type (insert/update/delete), and the specific data values that were changed.</p>
                    <ul>
                        <li><em>Pros:</em> Captures absolutely everything (including deletions). Triggers are free, built-in to any DB, use standard SQL, and are far easier for developers to implement without external tools.</li>
                        <li><em>Cons:</em> Slows down every data entry. It slows the source database due to the extra write operations required.</li>
                    </ul>
                </div>

                <div class="subsection">
                    <span class="subsection-title">Log-Based (Performance)</span>
                    <p><strong>Mechanism:</strong> Works at the storage layer: it parses the immutable binary transaction logs (WAL) that the database generates anyway for recovery.</p>
                    <ul>
                        <li><em>Pros:</em> The gold standard. It reads background files, capturing everything instantly without ever slowing the main system. It adds no extra SQL execution or load to the source database.</li>
                        <li><em>Cons:</em> Log parsing is complex, expensive, and vendor-specific.</li>
                    </ul>
                </div>

                <div class="subsection">
                    <span class="subsection-title">Time-Based (Simplicity)</span>
                    <p><strong>Mechanism:</strong> Source tables use a "Last Modified" column. The target saves the previous sync time (High Water Mark) and queries for records newer than that timestamp.</p>
                    <ul>
                        <li><em>Pros:</em> It's easy to build (just a query).</li>
                        <li><em>Cons:</em> Misses deletions and slows the database while checking. This captures inserts and updates but fails to detect deletions, as the record vanishes from the table before the scan.</li>
                    </ul>
                </div>
            </div>
        </details>

        <details class="section">
            <summary>5. Training Parameters</summary>
            <div class="section-content">
                
                <div class="subsection">
                    <span class="subsection-title">Schedulers</span>
                    <p><strong>Linear Scheduler:</strong> A <strong>Linear Scheduler</strong> drops the learning rate at a fixed, constant rate. After warmup, it follows a straight line to zero, offering predictable, steady cooling as the model trains.</p>
                    <p><strong>Cosine Scheduler:</strong> A <strong>Cosine Scheduler</strong> follows a bell-curve decay. It stays high longer, then drops fast before a very slow "tail" at the end, allowing for extremely precise fine-tuning in the final steps.</p>
                </div>

                <div class="subsection">
                    <span class="subsection-title">Batch Size</span>
                    <p><strong>Batch size</strong> is the number of data samples processed before the model updates its internal weights. Larger batches provide smoother, more accurate gradient estimates but demand high VRAM. If memory is tight, <strong>gradient accumulation</strong> mimics large batches by summing gradients over multiple small "mini-batches" before updating.</p>
                </div>

                <div class="subsection">
                    <span class="subsection-title">Maximum Length and Packing</span>
                    <p><strong>Maximum Sequence Length</strong> defines the token limit for a single input. Anything longer is truncated, while shorter inputs are often "padded" with empty tokens to match the length, which wastes VRAM.</p>
                    <p><strong>Packing</strong> (or "Multipack") fixes this inefficiency by stitching multiple short examples into a single sequence up to the maximum length. Instead of processing 500 tokens of data and 1500 tokens of useless padding, packing fills that slot with other real samples. To prevent the model from confusing these unrelated texts, <strong>attention masks</strong> are used to "wall off" each sample, ensuring the model only looks at tokens within the same original context.</p>
                </div>

                <div class="subsection">
                    <span class="subsection-title">Optimizers</span>
                    <p><strong>AdamW (8-bit & 32-bit):</strong> <strong>AdamW</strong> is the gold standard for LLMs. It uses "moments"—a memory of past gradients—to smooth updates and applies <strong>weight decay</strong> to prevent overfitting. While the 32-bit version is most precise, the <strong>8-bit version</strong> quantizes these moments to save massive amounts of VRAM. It provides the same convergence quality as full precision but allows you to train much larger models on consumer hardware.</p>
                    <p><strong>Paged & Standard Torch Optimizers — QLoRA Optimizers:</strong> <strong>Paged Optimizers</strong> (like Paged AdamW) act as a safety net; they offload optimizer states to CPU RAM when the GPU VRAM is full, preventing "Out of Memory" crashes. Conversely, <strong>adamw_torch</strong> is the uncompressed, 32-bit baseline. It offers the highest mathematical precision and is the best choice if your hardware has enough VRAM to handle the full weight of the optimizer's state without quantization.</p>
                </div>

            </div>
        </details>

    </div>
</div>

</body>
</html>