
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Part VI: LLM Evaluation // Study Guide</title>
<style>
    :root {
        --bg-color: #000000;
        --text-color: #00ff41;
        --accent-color: #00ff41;
        --dim-color: #003b00;
        --border-color: #00ff41;
        --font-main: 'Courier New', Courier, monospace;
        --font-header: 'Arial Black', Impact, sans-serif;
    }

    * { box-sizing: border-box; }

    body {
        margin: 0;
        padding: 0;
        background-color: var(--bg-color);
        color: var(--text-color);
        font-family: var(--font-main);
        line-height: 1.5;
        overflow-x: hidden;
    }

    /* --- VISUALS --- */
    .dither-layer {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        z-index: -1;
        background-image: radial-gradient(circle, #003b00 1px, transparent 1px);
        background-size: 4px 4px;
        opacity: 0.4;
    }

    .scanlines {
        position: fixed;
        top: 0; left: 0; width: 100%; height: 100%;
        background: linear-gradient(to bottom, rgba(0, 255, 65, 0), rgba(0, 255, 65, 0) 50%, rgba(0, 20, 0, 0.2) 50%, rgba(0, 20, 0, 0.2));
        background-size: 100% 4px;
        pointer-events: none;
        z-index: 9999;
    }

    .container {
        max-width: 900px;
        width: 100%;
        margin: 0 auto;
        padding: 40px 20px;
        border-left: 2px dashed var(--dim-color);
        border-right: 2px dashed var(--dim-color);
        background-color: rgba(0, 10, 0, 0.9);
        min-height: 100vh;
    }

    /* --- TYPOGRAPHY --- */
    h1 {
        font-family: var(--font-header);
        text-transform: uppercase;
        font-size: 2.5rem;
        border-bottom: 5px solid var(--accent-color);
        margin-bottom: 40px;
        color: var(--accent-color);
        text-align: center;
        text-shadow: 0px 0px 8px var(--accent-color);
        word-wrap: break-word;
    }

    h3 { margin-top: 0; color: var(--accent-color); text-transform: uppercase; }
    strong { color: var(--accent-color); text-decoration: underline; }
    em { font-style: normal; color: #50c878; border-bottom: 1px dotted var(--dim-color); }

    /* --- ACCORDION STYLES --- */
    /* Outer Parts */
    details.part {
        margin-bottom: 30px;
        border: 2px solid var(--border-color);
        background: #000;
        box-shadow: 6px 6px 0px var(--dim-color);
        transition: transform 0.1s;
    }
    details.part[open] { box-shadow: 4px 4px 0px var(--dim-color); transform: translate(2px, 2px); }
    details.part > summary {
        font-family: var(--font-header);
        font-size: 1.5rem;
        padding: 15px 20px;
        background-color: var(--accent-color);
        color: var(--bg-color);
        cursor: pointer;
        list-style: none;
        text-transform: uppercase;
        position: relative;
    }
    details.part > summary::-webkit-details-marker { display: none; }
    details.part > summary::after { content: '+'; position: absolute; right: 20px; font-weight: 900; }
    details.part[open] > summary::after { content: '-'; }
    .part-content { padding: 20px; border-top: 2px solid var(--border-color); }

    /* Inner Sections */
    details.section {
        margin-bottom: 15px;
        border: 1px solid var(--dim-color);
        background: #050505;
    }
    details.section > summary {
        font-family: var(--font-main);
        font-weight: bold;
        padding: 12px;
        background: #0a0a0a;
        color: var(--text-color);
        cursor: pointer;
        list-style: none;
        border-bottom: 1px solid transparent;
        text-transform: uppercase;
        font-size: 1.1rem;
    }
    details.section > summary:hover { background: var(--dim-color); color: var(--accent-color); }
    details.section[open] > summary {
        border-bottom: 1px solid var(--dim-color);
        background: #0f0f0f;
        color: var(--accent-color);
        text-shadow: 0px 0px 5px var(--accent-color);
    }
    .section-content { padding: 20px; }

    /* Subsections */
    .subsection {
        margin-bottom: 25px;
        border-left: 4px solid var(--dim-color);
        padding-left: 15px;
    }
    .subsection-title {
        background: var(--dim-color);
        color: var(--accent-color);
        padding: 2px 6px;
        font-weight: bold;
        text-transform: uppercase;
        display: inline-block;
        margin-bottom: 10px;
        font-size: 0.9rem;
    }

    p { margin-bottom: 12px; margin-top: 0; text-align: justify; }
    ul { padding-left: 20px; margin-bottom: 15px; }
    li { margin-bottom: 5px; }

    .code-block {
        background: #020a02;
        border: 1px dashed var(--dim-color);
        padding: 10px;
        margin: 10px 0;
        font-family: 'Courier New', monospace;
        color: var(--accent-color);
        overflow-x: auto;
        white-space: pre-wrap;
    }

    /* --- INTERACTIVE: EYE BUTTON & VIEWPORT --- */
    
    .eye-btn {
        position: relative;
        width: 28px; height: 28px;
        background: #000;
        border: 1px solid var(--accent-color);
        cursor: pointer;
        padding: 4px;
        display: inline-flex;
        align-items: center; justify-content: center;
        margin-left: 8px;
        vertical-align: bottom;
        transition: transform 0.1s;
    }
    .eye-btn svg { width: 100%; height: 100%; fill: var(--accent-color); }
    .eye-btn:hover {
        background: var(--accent-color);
        transform: translate(-1px, -1px);
        box-shadow: 2px 2px 0px var(--dim-color);
    }
    .eye-btn:hover svg { fill: #000; }

    .dither-bg {
        background-image: 
            linear-gradient(45deg, var(--dim-color) 25%, transparent 25%), 
            linear-gradient(-45deg, var(--dim-color) 25%, transparent 25%), 
            linear-gradient(45deg, transparent 75%, var(--dim-color) 75%), 
            linear-gradient(-45deg, transparent 75%, var(--dim-color) 75%);
        background-size: 4px 4px;
    }

    .retro-viewport {
        position: fixed;
        top: 50%; left: 50%;
        transform: translate(-50%, -50%);
        width: 80vw; height: 80vh;
        max-width: 900px; max-height: 700px;
        background-color: #000;
        border: 2px solid var(--accent-color);
        box-shadow: 0 0 50px rgba(0, 50, 0, 0.8);
        display: flex; flex-direction: column;
        z-index: 10000;
        visibility: hidden; opacity: 0;
        pointer-events: none;
        transition: opacity 0.2s;
        resize: both; overflow: hidden;
    }
    .retro-viewport.active { visibility: visible; opacity: 1; pointer-events: auto; }

    .vp-header {
        background: var(--accent-color);
        color: #000;
        padding: 5px 10px;
        font-weight: bold;
        font-family: var(--font-header);
        display: flex; justify-content: space-between;
        align-items: center;
        border-bottom: 2px solid #000;
        cursor: default;
    }
    .vp-close {
        background: #000; color: var(--accent-color);
        border: 1px solid #000; font-weight: 900; 
        cursor: pointer; font-family: var(--font-main);
    }
    .vp-close:hover { background: #fff; color: #000; }
    
    .vp-body { flex-grow: 1; position: relative; background: #000; }
    .vp-body iframe { width: 100%; height: 100%; border: none; }

    /* --- RESPONSIVE --- */
    @media (max-width: 600px) {
        h1 { font-size: 1.8rem; border-bottom-width: 3px; }
        details.part > summary { font-size: 1.1rem; padding: 12px; }
        details.section > summary { font-size: 0.9rem; }
        .container { padding: 10px; border: none; }
        .part-content, .section-content { padding: 10px; }
        p { text-align: left; }
        .retro-viewport { width: 95vw; height: 60vh; }
    }
</style>
</head>
<body>
<div class="dither-layer"></div>
<div class="scanlines"></div>
<div class="container">
<h1>Part VI: LLM Evaluation</h1>
code
Code
<div class="part-content">
    
    <details class="section">
        <summary>1. Evaluation Methodologies (Judges, RLHF, DPO)</summary>
        <div class="section-content">
            
            <div class="subsection">
                <span class="subsection-title">LLM-as-a-Judge: Scoring vs. Ranking</span>
                <p>When using an LLM to evaluate preference data, you have two primary methods:</p>
                <ul>
                    <li><strong>Absolute Scoring:</strong> The LLM assigns a 1–5 or "Good/Bad" rating to a single response. It’s fast but often inconsistent across different prompts or sessions.</li>
                    <li><strong>Pairwise Ranking [Best]:</strong> The LLM compares two responses (A vs. B) to pick a winner. This is the gold standard for DPO because it mimics human decision-making and produces more stable, relative signals.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Boosting Evaluation Accuracy</span>
                <p><strong>CoT + Pairwise Ranking:</strong> Use when the task is highly subjective or creative (e.g., poetry, persona-mimicry, or open-ended advice). Strength: Forces logical justification. Since no "answer key" exists, the judge's own reasoning process ensures it weighs complex trade-offs—like creativity vs. clarity—before picking a winner. To prevent the judge from relying on "vibes" or length, it generates its own independent reasoning to evaluate the candidates' final outputs. It does not read the candidate's thoughts; it builds its own logical path to a decision.</p>
                
                <p><strong>Ground Truth + Pairwise Ranking:</strong> Use when there is a definitive "correct" answer (e.g., Python scripts, calculus, or history). Strength: Acts as an immutable anchor. It eliminates hallucinations by providing the judge with the "answer key," forcing it to penalize confident but incorrect logic. Ground Truth acts as the "answer key" for the judge. It shifts the task from "Which sounds better?" to "Which is more correct?"</p>
                
                <p><strong>The Rubric + Pairwise Ranking:</strong> Use when the task is constrained by specific requirements but lacks a single "right" answer (e.g., "Summarize this for a 5-year-old"). Strength: Prevents "goalpost shifting." By defining the ideal criteria first, the judge resists favoring superficial traits like politeness or length over actual compliance. It acts as a stabilizing force, ensuring that the same criteria are applied to every pair in the dataset, leading to a much cleaner training signal for DPO.</p>
            </div>

            <div class="subsection">
                <span class="subsection-title">Reinforcement Learning from Human Feedback (RLHF)</span>
                <p>RLHF is most effective when the "ground truth" is difficult to define mathematically but easy for a human to recognize. The Bradley-Terry model turns a simple "A is better than B" vote into a mathematical probability. If a human prefers response $A$ over $B$, the loss function penalizes the model if its predicted score for $A$ isn't higher than its score for $B$.</p>
                <ul>
                    <li><strong>1. The Reward Model: Learning "Taste":</strong> The Reward Model (RM) is typically a sibling of the candidate model (same architecture) but modified with a scalar regression head. Instead of predicting the next word, it outputs a single number. Using a Bradley-Terry loss, the RM learns to distinguish the features—logic, tone, safety—that humans prefer.</li>
                    <li><strong>2. Policy Selection (PPO):</strong> Updates $\pi_\theta$ (parameters) via on-policy optimization. Its $KL-penalty$ mechanism enforces a constraint, providing a stable framework for aligning weights with the optimal policy.</li>
                    <li><strong>3. How the RM Informs the Candidate:</strong> The candidate model generates a response. The RM "grades" it with a score. PPO then calculates the Advantage: how much better this response was compared to the model's average. Positive score: The model is nudged to produce similar outputs. Negative score: The model is nudged away.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Direct Preference Optimization (DPO)</span>
                <p>Instead of training a separate Reward Model to "score" outputs, DPO uses the language model itself as the reward estimator. It compares the log probabilities of a "preferred" completion and a "rejected" completion relative to a static reference model. The training process uses a Binary Cross-Entropy (BCE) loss.</p>
                <p>$\beta$ determines the penalty for straying from the reference policy. A low $\beta$ prioritizes following preference data, risking $KL$-divergence and mode collapse. A high $\beta$ enforces a tighter "anchor," preserving the model's original distribution and reasoning.</p>
                
                <p><strong>Efficiency via Adapters:</strong> When paired with LoRA/QLoRA, DPO is exceptionally resource-efficient. Using adapters changes the architecture: Shared Weights (The large base model remains frozen and serves as the reference) and Trainable Modules (You only update small adapter layers). This effectively eliminates the memory overhead of the reference model while maintaining the KL-divergence constraint.</p>
            </div>

        </div>
    </details>

    <details class="section">
        <summary>2. Metrics & Benchmarks</summary>
        <div class="section-content">
            
            <div class="subsection">
                <span class="subsection-title">Simple Pre-Training Metrics</span>
                <p><strong>Training Loss:</strong> This measures the "error rate" while the model is learning. Using cross-entropy, it calculates how far the model's guess is from the actual next word. As the model improves, this number should consistently drop.</p>
                <p><strong>Validation Loss:</strong> The same error calculation, but performed on data the model has never seen during training. This is the ultimate test of "generalization".</p>
                <p><strong>Gradient Norm:</strong> This tracks the scale of internal updates. If the norm is too high (exploding) or too low (vanishing), the model is becoming unstable.</p>
                <p><strong>Perplexity:</strong> $PPL(X) = exp(- \frac{1}{t} \sum \log P(w_i | w_{<i}))$. A lower perplexity indicates the model is less "surprised" by natural language.</p>
            </div>

            <div class="subsection">
                <span class="subsection-title">Evaluations of Base Models</span>
                <ul>
                    <li><strong>MMLU (Knowledge):</strong> Measures broad factual knowledge in fields like law and medicine. Use this to see if a model has the "encyclopedic backbone" required for domain-specific tasks.</li>
                    <li><strong>HellaSwag (Reasoning):</strong> Tests situational common sense and narrative flow. It’s the go-to metric for chatbots that need to understand human intent and social context.</li>
                    <li><strong>GSM8K (Logic):</strong> Uses math word problems to assess multi-step reasoning. Prioritize this if your task requires complex instruction following.</li>
                    <li><strong>HumanEval (Coding):</strong> Measures Python proficiency and functional logic. Essential for technical tasks, software engineering, or structured data generation.</li>
                    <li><strong>ARC-C (Causality):</strong> Targets causal reasoning through hard science questions. It’s vital for tasks requiring "how" and "why" understanding.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Evaluations of Fine-Tuned Models</span>
                <p>Benchmarking fine-tuned models highlights the delta from the base version. Evaluations shift from testing raw "knowledge" to assessing how well the model behaves as an assistant.</p>
                <ul>
                    <li><strong>IFEval (Instruction Following Evaluation):</strong> Uses verifiable constraints to measure precision (e.g., "no commas" or "JSON format"). Ideal for apps requiring high format reliability and structural consistency.</li>
                    <li><strong>Chatbot Arena:</strong> The "gold standard" for human preference via blind, crowdsourced battles. High scores correlate with user satisfaction regarding a model's conversational style and tone.</li>
                    <li><strong>AlpacaEval:</strong> An automated Chatbot Arena simulator using a high-powered "judge" model (like GPT-4o). It is a fast, cost-effective development tool.</li>
                    <li><strong>MT-Bench (Multi-Turn Benchmark):</strong> Evaluates multi-turn conversations by testing context retention and consistency across follow-up questions.</li>
                    <li><strong>GAIA (General AI Assistants):</strong> Tests agentic skills through real-world tasks requiring tool use, browsing, and reasoning.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Domain Specific LLM Evaluations</span>
                <p>General benchmarks fail to capture the nuance of specialized models. To measure performance accurately, use specialized suites.</p>
                <ul>
                    <li><strong>MedQA (Healthcare):</strong> Tests medical reasoning via USMLE-style questions. It measures a model's ability to synthesize clinical data into accurate diagnoses and treatments.</li>
                    <li><strong>IaC-Eval (Infrastructure):</strong> Evaluates code generation for cloud infrastructure (e.g., Terraform).</li>
                    <li><strong>CAIBench (Defense):</strong> A meta-benchmark using containerized Capture The Flag tasks. It assesses tactical proficiency in reverse engineering and exploitation.</li>
                    <li><strong>RepoSpace (Aerospace):</strong> Focuses on repository-level code generation for spaceborne hardware.</li>
                    <li><strong>EngiBench (Engineering):</strong> A hierarchical benchmark for complex problem-solving in structural design.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Task Specific LLM Evaluations</span>
                <p>With a defined ground truth, you can use classic ML metrics to mathematically compare model predictions against a human-labeled "gold standard".</p>
                <ul>
                    <li><strong>ROUGE (Summarization):</strong> Focuses on "Recall." It measures how much of the original, important information is preserved by checking the overlap of word sequences (n-grams) between the model’s summary and a human reference.</li>
                    <li><strong>Accuracy (Classification):</strong> The simplest metric; it calculates the percentage of correct predictions out of the total.</li>
                    <li><strong>Precision (Quality):</strong> Measures the "trustworthiness" of positive results. It is the ratio of true positives to all positive predictions made. High precision means the model rarely labels something incorrectly.</li>
                    <li><strong>Recall (Quantity):</strong> Measures "completeness." It is the ratio of true positives to all actual positive instances in the data. High recall means the model successfully finds most of what it was supposed to catch.</li>
                    <li><strong>F1 Score (The Balance):</strong> The "harmonic mean" of precision and recall. It provides a single score that balances the trade-off between the two.</li>
                </ul>
            </div>

            <div class="subsection">
                <span class="subsection-title">Custom Benchmark Creation</span>
                <p><strong>Text Generation:</strong> The model is prompted to output a specific label (e.g., "B"). You use string matching or regex to verify the answer. This reflects real-world performance and tests the model's ability to follow strict formatting instructions.</p>
                <p><strong>Log-Likelihood:</strong> The system calculates the probability (logits) assigned to each answer token without generating text. The option with the highest likelihood is selected. This measures "raw" knowledge and avoids errors caused by verbose or malformed responses.</p>
            </div>

        </div>
    </details>

</div>
</div>
</body>
</html>